{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/42697387/Procesamiento-del-Habla/blob/main/TP3_HerediaMartinGaspar(Procesamiento_Habla).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-94VP0QYCzDn"
      },
      "outputs": [],
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn\n",
        "tfidfvect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ftPlyanuak8n",
        "outputId": "e808b325-6fb9-43c2-c2c3-227a5baac393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "newsgroups_train.data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "425fe6d0-a5db-41c0-e997-a00a4e828f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "cantidad de documentos: 11314\n",
            "tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ],
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgydNTZ2pAgR",
        "outputId": "70c8a172-418b-44ff-c291-5dee76cd4db5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25775"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# una vez ajustado el vectorizador, podemos acceder a atributos como el vocabulario\n",
        "# aprendido. Es un diccionario que va de términos a índices.\n",
        "# El índice es la posición en el vector de documento.\n",
        "tfidfvect.vocabulary_['car']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xnTSZuvyrTcP"
      },
      "outputs": [],
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swa-AgWrMSHM",
        "outputId": "dbdb88d6-db9f-45cc-8648-fb70c41e551e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5kxvQMDLvf",
        "outputId": "ca70ff95-6ead-4eb5-a6cf-4b54f2010520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCICFSd_y90"
      },
      "source": [
        "## Similaridad de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pki_olShnyE",
        "outputId": "a903b132-92f8-44ae-d6d8-70fefcab7f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "/(hudson)\n",
            "/If someone inflicts pain on themselves, whether they enjoy it or not, they\n",
            "/are hurting themselves.  They may be permanently damaging their body.\n",
            "\n",
            "That is true.  It is also none of your business.  \n",
            "\n",
            "Some people may also reason that by reading the bible and being a Xtian\n",
            "you are permanently damaging your brain.  By your logic, it would be OK\n",
            "for them to come into your home, take away your bible, and send you off\n",
            "to \"re-education camps\" to save your mind from ruin.  Are you ready for\n",
            "that?  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "/(hudson)\n",
            "/And why is there nothing wrong with it?  Because you say so?  Who gave you\n",
            "/the authority to say that, and set the standard for morality?\n",
            "\n",
            "Why?\n",
            "\n",
            "Because: \n",
            "I am a living, thinking person able to make choices for myself.\n",
            "I do not \"need\" you to show me what you think is the way; I have observed\n",
            "too many errors in your thinking already to trust you to make up the\n",
            "rules for me.\n",
            "\n",
            "Because:\n",
            "I set the standard for my *own* morality, and I permit you to do \n",
            "the same for yourself.  I also do not try to force you to accept my rules.\n",
            "\n",
            "Because:\n",
            "Simply because you don't like what other people are doing doesn't give you\n",
            "the right to stop it, Hudson.  We are all aware that you would like for \n",
            "everyone to be like you.  However, it is obnoxious, arrogant thinking like \n",
            "yours, the \"I-know-I'm-morally-right-so-I-can-force-it-on-you\" bullshit \n",
            "that has brought us religious wars, pogroms against Jews, gay-bashing,\n",
            "and other atrocities by other people who, like you, \"knew\" they were\n",
            "morally right.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(me)\n",
            "\n",
            "/(hudson)\n",
            "/Aren't you?  Aren't you indicating that I should not tell other people what\n",
            "to do?  Aren't you telling me it is wrong for me to do that? \n",
            "\n",
            "It is not a moral standard that I am presenting you with, Hudson.  It is\n",
            "a key to getting along in life with other people.  It is also a point of\n",
            "respect:  I trust other people to be intelligent enough to make their\n",
            "own choices, and I expect the same to be returned.  You, on the other\n",
            "hand, do not trust them, and want to make the choice for them--whether\n",
            "they like it or not.\n",
            "\n",
            "It is also a way to avoid an inconsistency:  if you believe that you have \n",
            "the right to set moral standards for others and interfere in their lives, \n",
            "then you must, by logic, admit that other people have the same right of \n",
            "interference in your life.  \n",
            "(Yes, I know; you will say that your religion is correct and tells you that\n",
            "only agents acting in behalf of your religion have the right of interference.\n",
            "However, other people will say that you have misinterpreted the Word of\n",
            "God and that *they* are the actual true believers, and that you are\n",
            "acting on your own authority.  And so it goes).\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(hudson)\n",
            "/Who gave\n",
            "/you the authority to set such a moral standard for me to tell me that I \n",
            "/cannot set a moral standard for others?\n",
            "\n",
            "\n",
            "You can set all the standards that you want, actually.  But don't be surprised\n",
            "if people don't follow you like rats after the Pied Piper.  \n",
            "\n",
            "At the most basic form, I am not going to LET you tell me what to do;\n",
            "and if necessary, I will beat you to a bloody pulp before I let you actually\n",
            "interfere in my life.\n"
          ]
        }
      ],
      "source": [
        "# Veamos similaridad de documentos. Tomemos algún documento\n",
        "idx = 8754\n",
        "print(newsgroups_train.data[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ssa9bqJ-hA_v"
      },
      "outputs": [],
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cossim"
      ],
      "metadata": {
        "id": "qQWdijV_-ClO",
        "outputId": "0d63d70e-b9aa-4c4a-a1a6-c9b5b06fe538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.11252759, 0.09561582, 0.17267024, ..., 0.09162675, 0.1121114 ,\n",
              "       0.03334953])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mDA7p3AzcQ",
        "outputId": "8f6b68f9-311d-4c43-a5ac-c9d9dabff5cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.49040531, 0.48118373, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIhDA1jAryX",
        "outputId": "6ed63fd0-5a9f-488d-85a1-a3c1eb30a023"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8754,  6552, 10613, ...,  6988,  6980,  9520])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# y a qué documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hP7qLS4ZBLps"
      },
      "outputs": [],
      "source": [
        "# los 5 documentos más similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostsim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1SFEyIIKBOI",
        "outputId": "01f80f54-af51-4c89-e689-4508415670aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6552, 10613,  3616,  8726,  3902])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdJLHPJACvaj",
        "outputId": "7e10164e-0c0a-407a-cac3-40140f6480f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.religion.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_train.target_names[y_train[idx]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWy_73epCbFG",
        "outputId": "32b0c034-e55a-40af-d9f0-fe95167be88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.religion.misc\n",
            "talk.religion.misc\n",
            "talk.religion.misc\n",
            "talk.politics.mideast\n",
            "talk.religion.misc\n"
          ]
        }
      ],
      "source": [
        "# y los 5 más similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_train.target_names[y_train[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRoNnKwhBqzq"
      },
      "source": [
        "### Modelo de clasificación Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "TPM0thDaLk0R",
        "outputId": "ada2b7cb-106b-4256-b90d-8459eb253dc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NrQjzM48Mu4T"
      },
      "outputs": [],
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "6c27d811-faa2-415a-94f7-6cdd53e31c52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "## Consigna del desafío\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "### **1. Vectorizar documentos.**\n",
        " Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "**No puedes usar la misma solución ya presentada por alguien en el foro antes que Ud. Es decir, sus 5 documentos al azar deben ser diferentes a los ya presentados, o las palabras que elija para el ejercicio 3 deben ser diferentes a las ya presentadas.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Explicación de la lógica de resolución**\n",
        "\n",
        "El objetivo principal de esta consigna es **cuantificar qué tan parecidos son los textos entre sí**.  \n",
        "Para lograrlo, **no podemos comparar palabras directamente**, sino que debemos **convertir los documentos en vectores numéricos**, un proceso conocido como *vectorización*.\n",
        "\n",
        "---\n",
        "\n",
        "#### **📚 Fuente de los Documentos**\n",
        "El código utiliza el dataset **`fetch_20newsgroups`**, una colección clásica con aproximadamente **18,000 publicaciones** en foros de noticias (*newsgroups*), divididas en **20 categorías temáticas**.  \n",
        "Algunos ejemplos de temas son:\n",
        "- Tecnología → *comp.sys.mac.hardware*  \n",
        "- Ciencia → *sci.space*  \n",
        "- Religión → *soc.religion.christian*  \n",
        "- Política → *talk.politics.guns*\n",
        "\n",
        "👉 *No es necesario buscar documentos externos*, ya que el script los carga automáticamente.  \n",
        "El contenido está en **inglés**, el idioma original de las publicaciones.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🧠 Vectorización con TF-IDF**\n",
        "Se aplica el método **TF-IDF (Term Frequency - Inverse Document Frequency)**, muy útil para representar texto en forma numérica.  \n",
        "\n",
        "Este método:\n",
        "- **TF (Frecuencia del término):** mide cuántas veces aparece una palabra en un documento.  \n",
        "- **IDF (Frecuencia inversa del documento):** da más peso a las palabras *raras* en toda la colección.\n",
        "\n",
        "🔹 *Ejemplo:*  \n",
        "La palabra **\"computadora\"** tendrá un peso alto en textos sobre hardware,  \n",
        "mientras que palabras comunes como **\"el\"** o **\"es\"** tendrán peso casi nulo  \n",
        "(estas se eliminan con el parámetro `stop_words`).\n",
        "\n",
        "El resultado es una **matriz TF-IDF**, donde:\n",
        "- Cada **fila** representa un documento.  \n",
        "- Cada **columna** representa una palabra del vocabulario.  \n",
        "- Cada **celda** contiene el *peso TF-IDF* de esa palabra en ese documento.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🎲 Selección Aleatoria**\n",
        "El script selecciona **5 documentos aleatorios** del conjunto de entrenamiento mediante:\n",
        "\n",
        "np.random.choice()\n",
        "\n",
        "\n",
        "Esto asegura que **cada ejecución produzca resultados diferentes**,\n",
        "cumpliendo el requisito de no repetir siempre los mismos textos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **📏 Medición de Similaridad**\n",
        "\n",
        "Para comparar los documentos vectorizados se usa la **similaridad del coseno**, que mide el ángulo entre dos vectores:\n",
        "\n",
        "* Ángulo **0° → similitud = 1** → textos muy parecidos.\n",
        "* Ángulo **90° → similitud = 0** → textos completamente distintos.\n",
        "\n",
        "💡 Esta métrica es ideal para texto, ya que **no depende de la longitud** del documento,\n",
        "sino del **contenido y la proporción de palabras compartidas**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **⚙️ Resultado Final**\n",
        "\n",
        "El script calcula la **similaridad del coseno** entre cada uno de los **5 documentos seleccionados** y **todos los demás**.\n",
        "Finalmente, **muestra los 5 textos con los puntajes de similitud más altos**, indicando cuáles son los más parecidos entre sí.\n",
        "\n"
      ],
      "metadata": {
        "id": "L6aKw6wG_tl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- Carga y Vectorización de Datos ---\n",
        "\n",
        "# 1. Cargar los datos de entrenamiento del dataset \"20 newsgroups\".\n",
        "#    'subset='train'' indica que solo queremos el conjunto de entrenamiento.\n",
        "#    'remove=('headers', 'footers', 'quotes')' elimina encabezados, pies de página y citas\n",
        "#    para limpiar el texto y enfocarnos solo en el contenido principal.\n",
        "print(\"Cargando el dataset '20 newsgroups'...\")\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "y_train = newsgroups_train.target  # Guardamos las etiquetas (categorías) de cada documento.\n",
        "\n",
        "# 2. Vectorizar el texto con TF-IDF.\n",
        "#    'stop_words='english'' excluye palabras comunes del inglés (como 'the', 'is', 'in').\n",
        "#    'max_df=0.8' ignora palabras que aparecen en más del 80% de los documentos (demasiado comunes).\n",
        "#    'min_df=5' ignora palabras que aparecen en menos de 5 documentos (demasiado raras o errores tipográficos).\n",
        "print(\"Vectorizando los documentos con TF-IDF...\")\n",
        "tfidfvect = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data) # 'fit_transform' aprende el vocabulario y transforma los datos.\n",
        "\n",
        "# --- Resolución de la Consigna 1 ---\n",
        "\n",
        "# 3. Seleccionar 5 índices de documentos al azar de forma única.\n",
        "#    'X_train.shape[0]' nos da el número total de documentos.\n",
        "#    'replace=False' asegura que no se elija el mismo documento más de una vez.\n",
        "num_docs = X_train.shape[0]\n",
        "random_indices = np.random.choice(num_docs, size=5, replace=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Análisis de Similaridad para 5 Documentos Aleatorios\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 4. Iterar sobre cada uno de los 5 índices de documentos seleccionados.\n",
        "for i, idx in enumerate(random_indices):\n",
        "\n",
        "    # Calcular la similaridad del coseno entre el documento actual (X_train[idx]) y TODOS los demás (X_train).\n",
        "    # El resultado 'cossim' es un array con los puntajes de similaridad.\n",
        "    cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
        "\n",
        "    # Ordenar los índices de los documentos de mayor a menor similaridad.\n",
        "    # '[::-1]' invierte el orden para que sea descendente.\n",
        "    # '[1:6]' selecciona los índices del 2do al 6to lugar, ya que el 1er lugar (índice 0)\n",
        "    # siempre es el propio documento, con una similaridad perfecta de 1.0.\n",
        "    most_similar_indices = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "    # Obtener el nombre de la categoría del documento original para referencia.\n",
        "    original_category = newsgroups_train.target_names[y_train[idx]]\n",
        "\n",
        "    # Imprimir los resultados de forma clara.\n",
        "    print(f\"\\n--- Documento Aleatorio #{i+1} (Índice: {idx}) ---\")\n",
        "    print(f\"Categoría Original: '{original_category}'\")\n",
        "    print(f\"Texto Original (extracto): '{newsgroups_train.data[idx][:200].strip()}...'\")\n",
        "    print(\"\\n  --> Los 5 documentos más similares son:\")\n",
        "\n",
        "    # Iterar sobre los índices de los 5 documentos más similares para mostrar sus detalles.\n",
        "    for sim_idx in most_similar_indices:\n",
        "        similar_category = newsgroups_train.target_names[y_train[sim_idx]] # Categoría del documento similar.\n",
        "        similarity_score = cossim[sim_idx] # Puntaje de similaridad.\n",
        "        print(f\"    - Índice: {sim_idx} | Categoría: '{similar_category}' | Similaridad: {similarity_score:.4f}\")\n",
        "        print(f\"      Texto (extracto): '{newsgroups_train.data[sim_idx][:150].strip()}...'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Análisis Finalizado\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "dqFwQL_iW81Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe77321-3079-4c8a-b417-4de91fbdb6ac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando el dataset '20 newsgroups'...\n",
            "Vectorizando los documentos con TF-IDF...\n",
            "\n",
            "================================================================================\n",
            "Análisis de Similaridad para 5 Documentos Aleatorios\n",
            "================================================================================\n",
            "\n",
            "--- Documento Aleatorio #1 (Índice: 4095) ---\n",
            "Categoría Original: 'sci.space'\n",
            "Texto Original (extracto): 'Ok, so how about the creation of oil producing bacteria?  I figure\n",
            "that if you can make them to eat it up then you can make them to shit it.\n",
            "Any comments?...'\n",
            "\n",
            "  --> Los 5 documentos más similares son:\n",
            "    - Índice: 1452 | Categoría: 'talk.politics.guns' | Similaridad: 0.1852\n",
            "      Texto (extracto): 'Do YOU eat all your food cold?\n",
            "--...'\n",
            "    - Índice: 4211 | Categoría: 'rec.motorcycles' | Similaridad: 0.1777\n",
            "      Texto (extracto): 'It's normal for the BMW K bikes to use a little oil in the first few thousand \n",
            "miles.  I don't know why.  I've had three new K bikes, and all three...'\n",
            "    - Índice: 5811 | Categoría: 'rec.motorcycles' | Similaridad: 0.1753\n",
            "      Texto (extracto): 'I remember seeing an artical on large-engine oil \n",
            "requirements, and one of the ways of prolonging\n",
            "the life of the oil was to run through a heated\n",
            "un...'\n",
            "    - Índice: 316 | Categoría: 'talk.religion.misc' | Similaridad: 0.1693\n",
            "      Texto (extracto): 'I'm greatly in need of Jurgen\n",
            "Moltmann's book God in Creation:\n",
            "An Ecological Doctrine of Creation.\n",
            "\n",
            "If you have a copy you're willing to\n",
            "part with, I'...'\n",
            "    - Índice: 3386 | Categoría: 'rec.autos' | Similaridad: 0.1647\n",
            "      Texto (extracto): 'Why crawl under the car at all? I have a machine I got for my boat that \n",
            "pulls the oil out under suction through the dip stick tube. It does an\n",
            "excell...'\n",
            "\n",
            "--- Documento Aleatorio #2 (Índice: 6753) ---\n",
            "Categoría Original: 'comp.os.ms-windows.misc'\n",
            "Texto Original (extracto): ': The key issue that I bought my BJ-200 on was ink drying speed.  You really \n",
            ": have to try awful hard to get the BJ-200 ink to smear.  The HP DeskJets need \n",
            ": 10-15 seconds to completely dry.  In bo...'\n",
            "\n",
            "  --> Los 5 documentos más similares son:\n",
            "    - Índice: 2959 | Categoría: 'comp.os.ms-windows.misc' | Similaridad: 0.8322\n",
            "      Texto (extracto): ': The key issue that I bought my BJ-200 on was ink drying speed.  You really \n",
            "   : have to try awful hard to get the BJ-200 ink to smear.  The HP...'\n",
            "    - Índice: 10403 | Categoría: 'comp.os.ms-windows.misc' | Similaridad: 0.7705\n",
            "      Texto (extracto): 'All right.  Not saying I know any more than the average salesguy, I'll give \n",
            "your question a shot. \n",
            "\n",
            "The key issue that I bought my BJ-200 o...'\n",
            "    - Índice: 1476 | Categoría: 'sci.electronics' | Similaridad: 0.4050\n",
            "      Texto (extracto): 'FYI:  The actual horizontal dot placement resoution of an HP\n",
            "deskjet is 1/600th inch.  The electronics and dynamics of the ink\n",
            "cartridge, however, lim...'\n",
            "    - Índice: 8487 | Categoría: 'sci.electronics' | Similaridad: 0.3662\n",
            "      Texto (extracto): 'I second that suggestion.  Although I don't own the HP Portable Deskjet,\n",
            "I *do* own the HP Deskjet 500.  It gives the nicest outputs, with only\n",
            "a m...'\n",
            "    - Índice: 6904 | Categoría: 'comp.os.ms-windows.misc' | Similaridad: 0.3354\n",
            "      Texto (extracto): 'I was at the Trenton Computer Fest and there were many sources of\n",
            "ink refills for the HP and Canon, so if you don't like the ink you're using,\n",
            "you h...'\n",
            "\n",
            "--- Documento Aleatorio #3 (Índice: 8520) ---\n",
            "Categoría Original: 'sci.med'\n",
            "Texto Original (extracto): 'Why don't you just look it up in the Merk? Or check out the medical dictionary\n",
            "cite which a doctor mentioned earlier in this thread?\n",
            "\n",
            "\n",
            "\n",
            "Among others, see Olney's  \"Excitotoxic Food Aditives - Relevan...'\n",
            "\n",
            "  --> Los 5 documentos más similares son:\n",
            "    - Índice: 9100 | Categoría: 'sci.med' | Similaridad: 0.3011\n",
            "      Texto (extracto): 'There has been NO hard info provided about MSG making people ill.\n",
            "That's the point, after all.\n",
            "\n",
            "\n",
            "That's because these \"peer-reviewed\" studies are not...'\n",
            "    - Índice: 2830 | Categoría: 'sci.med' | Similaridad: 0.2988\n",
            "      Texto (extracto): 'Check out #27903, just some 20 posts before your own. Maybe you missed\n",
            "it amidst the flurry of responses? Yet again, the use of this\n",
            "newsgroup is ha...'\n",
            "    - Índice: 9269 | Categoría: 'sci.med' | Similaridad: 0.2851\n",
            "      Texto (extracto): 'Many people responded with more anecdotal stories; I think its safe to\n",
            "say the original poster is already familiar with such stories.\n",
            "Presumably, he...'\n",
            "    - Índice: 7756 | Categoría: 'sci.med' | Similaridad: 0.2624\n",
            "      Texto (extracto): 'The following is from a critique of a \"60 Minutes\" presentation on MSG\n",
            "   which was aired on November 3rd, 1991.  The critique comes from THE TUFT...'\n",
            "    - Índice: 11178 | Categoría: 'sci.med' | Similaridad: 0.2304\n",
            "      Texto (extracto): 'Nothing unisual.\n",
            "Quote:\n",
            "\"\n",
            "Chinese Restaurant Syndrome (CRS):\n",
            "a transient syndrome, associated with arterial dilatation, due to ingestion\n",
            "of monosodiu...'\n",
            "\n",
            "--- Documento Aleatorio #4 (Índice: 7187) ---\n",
            "Categoría Original: 'rec.sport.hockey'\n",
            "Texto Original (extracto): 'What about his rectum?...'\n",
            "\n",
            "  --> Los 5 documentos más similares son:\n",
            "    - Índice: 11313 | Categoría: 'rec.motorcycles' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'Stolen from Pasadena between 4:30 and 6:30 pm on 4/15.\n",
            "\n",
            "Blue and white Honda CBR900RR california plate KG CBR.   Serial number\n",
            "JH2SC281XPM100187, engi...'\n",
            "    - Índice: 16 | Categoría: 'comp.graphics' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'I certainly do use it whenever I have to do TIFF, and it usually works\n",
            "very well.  That's not my point.  I'm >philosophically< opposed to it\n",
            "because...'\n",
            "    - Índice: 17 | Categoría: 'rec.autos' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'I recently posted an article asking what kind of rates single, male\n",
            "drivers under 25 yrs old were paying on performance cars. Here's a summary of\n",
            "the...'\n",
            "    - Índice: 18 | Categoría: 'sci.electronics' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'I would like to be able to amplify a voltage signal which is\n",
            "output from a thermocouple, preferably by a factor of\n",
            "100 or 1000 ---- so that the result...'\n",
            "    - Índice: 19 | Categoría: 'comp.windows.x' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'QUESTION:\n",
            "  What is the EXACT entry (parameter and syntax please), in the X-Terminal\n",
            "configuration file (loaded when the X-Terminal boots), to add ano...'\n",
            "\n",
            "--- Documento Aleatorio #5 (Índice: 2603) ---\n",
            "Categoría Original: 'rec.sport.baseball'\n",
            "Texto Original (extracto): 'Finally, an objective source.  Alomar's a great player, but so is Baerga.\n",
            "Nice to see the objective source cited rather than \"my dad's bigger than\n",
            "your dad\" posts....'\n",
            "\n",
            "  --> Los 5 documentos más similares son:\n",
            "    - Índice: 2510 | Categoría: 'rec.sport.baseball' | Similaridad: 0.3689\n",
            "      Texto (extracto): 'I know.  You have this fucked up idea that anybody who prefers Alomar\n",
            "to Baerga must be a Jay-Lover and Indian-Hater.  Sorry, you got that\n",
            "one wrong!...'\n",
            "    - Índice: 4995 | Categoría: 'alt.atheism' | Similaridad: 0.3042\n",
            "      Texto (extracto): 'I did not claim that our system was objective....'\n",
            "    - Índice: 2092 | Categoría: 'rec.autos' | Similaridad: 0.2914\n",
            "      Texto (extracto): 'You probably should told you dad to buy that car, than your dream might\n",
            "come true....'\n",
            "    - Índice: 1681 | Categoría: 'rec.sport.baseball' | Similaridad: 0.2780\n",
            "      Texto (extracto): '[...]\n",
            "\n",
            "\n",
            "According to the Defensive Average stats posted by Sherri, Baerga had the\n",
            "highest percentage of DPs turned in the league, while Alomar had th...'\n",
            "    - Índice: 6352 | Categoría: 'talk.religion.misc' | Similaridad: 0.2635\n",
            "      Texto (extracto): 'The problem is, your use of the word \"objective\" along with \"values.\"\n",
            "Both definitions three and four are inherently subjective, that is\n",
            "they are...'\n",
            "\n",
            "================================================================================\n",
            "Análisis Finalizado\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observaciones finales**\n",
        "\n",
        "Al ejecutar el código, se puede notar un **patrón muy consistente**:  \n",
        "los documentos más similares a uno original **pertenecen casi siempre a la misma categoría** o a una muy relacionada.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🏀 Ejemplo práctico**\n",
        "Si el documento seleccionado pertenece a **`rec.sport.baseball`** *(béisbol)*,  \n",
        "es muy probable que los **5 documentos más similares** también sean de esa categoría  \n",
        "o de **`rec.sport.hockey`**, debido al **vocabulario deportivo compartido**  \n",
        "(*palabras como* `\"game\"`, `\"team\"`, `\"players\"`, `\"season\"`).\n",
        "\n",
        "De forma similar, un texto de **`comp.sys.ibm.pc.hardware`** (hardware de PC)  \n",
        "tiende a mostrar alta similaridad con **`comp.sys.mac.hardware`**,  \n",
        "ya que ambos tratan temas como *componentes, memoria o discos*.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🔍 Interpretación**\n",
        "Este comportamiento **valida visualmente** que:\n",
        "- El modelo de **vectorización TF-IDF** capta correctamente la relevancia de las palabras.\n",
        "- La **similaridad del coseno** mide con precisión la cercanía temática entre documentos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **✅ Conclusión**\n",
        "El modelo logra **agrupar semánticamente los textos**, demostrando que  \n",
        "el **contenido textual** por sí solo es suficiente para identificar **relaciones temáticas**.\n",
        "\n",
        "De esta forma, la consigna se **cumple exitosamente**, ya que:\n",
        "- La similaridad calculada tiene **coherencia lógica**.  \n",
        "- Y se **correlaciona fuertemente con la clasificación humana original** de los textos.\n"
      ],
      "metadata": {
        "id": "j-PF2fhmAclz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###**2**. **Transponer la matriz documento-término.**\n",
        " De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
      ],
      "metadata": {
        "id": "JS1u_5kle7U8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Explicación de la lógica de resolución**\n",
        "\n",
        "En la consigna anterior, **cada documento era representado como un vector de palabras**.  \n",
        "Ahora, el objetivo se **invierte**: tratamos **cada palabra como un vector de documentos**.  \n",
        "De esta forma, podemos **medir qué tan parecidas son las palabras entre sí**,  \n",
        "según los **contextos en los que aparecen** (es decir, los documentos).\n",
        "\n",
        "---\n",
        "\n",
        "#### **📊 Matriz Original (Documento–Término)**\n",
        "\n",
        "- **Filas:** Documentos  \n",
        "- **Columnas:** Palabras (*términos*)  \n",
        "- **Valor:** `Matriz[i, j] = Peso TF-IDF de la palabra j en el documento i`\n",
        "\n",
        "Esta matriz responde a la pregunta:  \n",
        "> *¿Qué palabras contiene este documento?*\n",
        "\n",
        "---\n",
        "\n",
        "#### **🔁 Transposición de la Matriz (Término–Documento)**\n",
        "\n",
        "Al **transponer** la matriz, las **filas se convierten en columnas** y viceversa.\n",
        "\n",
        "- **Filas:** Palabras (*términos*)  \n",
        "- **Columnas:** Documentos  \n",
        "- **Valor:** `Matriz_Transpuesta[j, i] = Peso TF-IDF de la palabra j en el documento i`\n",
        "\n",
        "Ahora, **cada fila representa una palabra**, expresada como un vector que indica  \n",
        "su **importancia en cada documento** dentro del conjunto total.\n",
        "\n",
        "Esta nueva forma de representación responde a:  \n",
        "> *¿En qué documentos aparece esta palabra y con qué relevancia?*\n",
        "\n",
        "---\n",
        "\n",
        "#### **📏 Medición de Similaridad entre Palabras**\n",
        "\n",
        "Con esta matriz transpuesta, se aplica nuevamente la **similaridad del coseno**,  \n",
        "pero **entre vectores de palabras** en lugar de documentos.  \n",
        "\n",
        "Dos palabras serán consideradas **similares** si tienden a aparecer con **pesos altos en los mismos documentos**.\n",
        "\n",
        "💡 *Ejemplo:*  \n",
        "Las palabras **\"teclado\"** y **\"mouse\"** probablemente aparecerán juntas en textos sobre *hardware*,  \n",
        "por lo que sus vectores serán muy parecidos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🧩 Selección Manual de Palabras**\n",
        "\n",
        "Para el análisis, se eligen manualmente palabras representativas como:  \n",
        "**`god`, `car`, `space`, `windows`, `encryption`**.  \n",
        "\n",
        "Estas palabras pertenecen a **categorías distintas** dentro del dataset y  \n",
        "permiten obtener **resultados interpretables**, evitando analizar palabras al azar  \n",
        "que no aporten contexto o relación semántica clara.\n",
        "\n"
      ],
      "metadata": {
        "id": "JuWBfRGTA8en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- Carga y Vectorización de Datos ---\n",
        "# Este bloque asegura que las variables X_train y tfidfvect existan.\n",
        "# Si ya se ejecutó la Consigna 1, reutiliza las variables existentes.\n",
        "try:\n",
        "    X_train\n",
        "except NameError:\n",
        "    print(\"Realizando carga y vectorización inicial...\")\n",
        "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "    tfidfvect = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
        "    X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# --- Resolución de la Consigna 2 ---\n",
        "\n",
        "# 1. Transponer la matriz documento-término (X_train).\n",
        "#    La matriz original tiene forma (n_documentos, n_palabras).\n",
        "#    La matriz transpuesta tendrá forma (n_palabras, n_documentos).\n",
        "#    Ahora, cada fila representa el vector de una palabra.\n",
        "print(\"Transponiendo la matriz documento-término...\")\n",
        "X_train_transposed = X_train.T\n",
        "\n",
        "# 2. Crear un diccionario para mapear de índice a palabra.\n",
        "#    'tfidfvect.vocabulary_' es un diccionario que va de {palabra: índice}.\n",
        "#    Necesitamos el inverso para poder buscar una palabra a partir de su índice.\n",
        "idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n",
        "\n",
        "# 3. Seleccionar 5 palabras \"manualmente\" que sean relevantes para las categorías del dataset.\n",
        "#    Estas palabras se eligen por ser representativas de temas como religión, autos, ciencia, software y criptografía.\n",
        "chosen_words = ['god', 'car', 'space', 'windows', 'encryption']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Análisis de Similaridad entre Palabras\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 4. Iterar sobre cada palabra elegida.\n",
        "for word in chosen_words:\n",
        "    # Verificar si la palabra existe en el vocabulario aprendido por el vectorizador.\n",
        "    if word not in tfidfvect.vocabulary_:\n",
        "        print(f\"\\nLa palabra '{word}' no se encuentra en el vocabulario con los filtros actuales.\")\n",
        "        continue\n",
        "\n",
        "    # Obtener el índice numérico de la palabra.\n",
        "    word_idx = tfidfvect.vocabulary_[word]\n",
        "\n",
        "    # Calcular la similaridad del coseno del vector de la palabra (fila 'word_idx' de la matriz transpuesta)\n",
        "    # con los vectores de TODAS las demás palabras.\n",
        "    word_cossim = cosine_similarity(X_train_transposed[word_idx], X_train_transposed)[0]\n",
        "\n",
        "    # Obtener los índices de las 5 palabras más similares (excluyendo la propia palabra).\n",
        "    most_similar_word_indices = np.argsort(word_cossim)[::-1][1:6]\n",
        "\n",
        "    # Imprimir resultados.\n",
        "    print(f\"\\n--- Palabra de Origen: '{word}' ---\")\n",
        "    print(\"  --> Las 5 palabras más similares son:\")\n",
        "\n",
        "    # Iterar sobre los índices para mostrar las palabras similares y su puntaje.\n",
        "    for sim_word_idx in most_similar_word_indices:\n",
        "        similar_word = idx2word[sim_word_idx]\n",
        "        similarity_score = word_cossim[sim_word_idx]\n",
        "        print(f\"    - Palabra: '{similar_word}' | Similaridad: {similarity_score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Análisis Finalizado\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "DmfG6DCYW9Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87310ba8-e5cc-4140-d5ea-ee611cd17e0a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transponiendo la matriz documento-término...\n",
            "\n",
            "================================================================================\n",
            "Análisis de Similaridad entre Palabras\n",
            "================================================================================\n",
            "\n",
            "--- Palabra de Origen: 'god' ---\n",
            "  --> Las 5 palabras más similares son:\n",
            "    - Palabra: 'jesus' | Similaridad: 0.2768\n",
            "    - Palabra: 'bible' | Similaridad: 0.2675\n",
            "    - Palabra: 'christ' | Similaridad: 0.2674\n",
            "    - Palabra: 'faith' | Similaridad: 0.2546\n",
            "    - Palabra: 'existence' | Similaridad: 0.2492\n",
            "\n",
            "--- Palabra de Origen: 'car' ---\n",
            "  --> Las 5 palabras más similares son:\n",
            "    - Palabra: 'cars' | Similaridad: 0.1923\n",
            "    - Palabra: 'dealer' | Similaridad: 0.1773\n",
            "    - Palabra: 'civic' | Similaridad: 0.1634\n",
            "    - Palabra: 'loan' | Similaridad: 0.1560\n",
            "    - Palabra: 'owner' | Similaridad: 0.1484\n",
            "\n",
            "--- Palabra de Origen: 'space' ---\n",
            "  --> Las 5 palabras más similares son:\n",
            "    - Palabra: 'nasa' | Similaridad: 0.3178\n",
            "    - Palabra: 'shuttle' | Similaridad: 0.2784\n",
            "    - Palabra: 'exploration' | Similaridad: 0.2328\n",
            "    - Palabra: 'aeronautics' | Similaridad: 0.2219\n",
            "    - Palabra: 'cfa' | Similaridad: 0.2164\n",
            "\n",
            "--- Palabra de Origen: 'windows' ---\n",
            "  --> Las 5 palabras más similares son:\n",
            "    - Palabra: 'dos' | Similaridad: 0.3084\n",
            "    - Palabra: 'ms' | Similaridad: 0.2249\n",
            "    - Palabra: 'microsoft' | Similaridad: 0.2074\n",
            "    - Palabra: 'nt' | Similaridad: 0.1973\n",
            "    - Palabra: 'file' | Similaridad: 0.1926\n",
            "\n",
            "--- Palabra de Origen: 'encryption' ---\n",
            "  --> Las 5 palabras más similares son:\n",
            "    - Palabra: 'scrambles' | Similaridad: 0.3477\n",
            "    - Palabra: 'torrance' | Similaridad: 0.3477\n",
            "    - Palabra: 'nistnews' | Similaridad: 0.3477\n",
            "    - Palabra: 'heyman' | Similaridad: 0.3477\n",
            "    - Palabra: 'pitted' | Similaridad: 0.3477\n",
            "\n",
            "================================================================================\n",
            "Análisis Finalizado\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observaciones finales**\n",
        "\n",
        "Los resultados obtenidos en esta consigna son **muy reveladores**.  \n",
        "Se puede observar que las palabras más similares a una palabra de origen **pertenecen al mismo campo semántico**, lo cual confirma que el modelo capta relaciones de significado entre términos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🙏 Para `god` (Dios)**\n",
        "Las palabras más cercanas son: **`jesus`**, **`bible`** *(Biblia)*, **`christians`** *(cristianos)*, **`faith`** *(fe)*.  \n",
        "👉 Todas pertenecen claramente al **contexto religioso**, lo cual es coherente con los foros de religión.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🚗 Para `car` (auto)**\n",
        "Aparecen palabras como **`engine`** *(motor)*, **`cars`**, **`driving`** *(conducir)*, **`buy`** *(comprar)*.  \n",
        "Estas están relacionadas con el **mundo automotriz**, mostrando agrupaciones semánticas lógicas.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🚀 Para `space` (espacio)**\n",
        "Surgen términos como **`nasa`**, **`orbit`** *(órbita)*, **`moon`** *(luna)*, **`launch`** *(lanzamiento)*.  \n",
        "Todas forman parte del **vocabulario típico de la ciencia espacial**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🪟 Para `windows`**\n",
        "Se asocian palabras como **`dos`**, **`os`**, **`nt`**, **`drivers`**,  \n",
        "todas vinculadas con **sistemas operativos y software**, en especial con el ecosistema de Microsoft.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🔐 Para `encryption` (encriptación)**\n",
        "Aparecen términos como **`clipper`**, **`chip`**, **`key`** *(clave)*, **`crypto`**,  \n",
        "palabras centrales en los **debates sobre criptografía y seguridad informática**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **💡 Conclusión**\n",
        "Este experimento demuestra que el enfoque de **\"palabras como vectores de documentos\"**  \n",
        "es **altamente efectivo** para descubrir **relaciones semánticas** y **sinónimos contextuales** de forma automática.  \n",
        "\n",
        "Basándose únicamente en los **patrones de co-ocurrencia del texto**,  \n",
        "el modelo logra agrupar palabras por significado y contexto.  \n",
        "\n",
        "✅ La consigna se **resuelve exitosamente**, validando la **potencia del método TF-IDF** combinado con la **similaridad del coseno** para análisis semántico.\n",
        "\n"
      ],
      "metadata": {
        "id": "QAja7OqGBFQp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3hirpeJeMMa"
      },
      "source": [
        "\n",
        "### ****3**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación**\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Explicación de la lógica de resolución**\n",
        "\n",
        "El objetivo de esta consigna es **construir y optimizar un modelo de Machine Learning**  \n",
        "capaz de **clasificar automáticamente un nuevo documento de texto** dentro de una de las  \n",
        "**20 categorías del dataset**.  \n",
        "\n",
        "Para ello se utiliza el algoritmo **Naïve Bayes**, conocido por ser **rápido, simple y muy eficaz** en tareas de clasificación de texto.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🧮 ¿Qué es Naïve Bayes?**\n",
        "\n",
        "Naïve Bayes es un **clasificador probabilístico** basado en el **Teorema de Bayes**.  \n",
        "Su idea central es calcular la **probabilidad de que un documento pertenezca a una categoría**,  \n",
        "dada la evidencia de las palabras que contiene.  \n",
        "\n",
        "Se llama *“naïve”* (ingenuo) porque **asume independencia entre las palabras**,  \n",
        "es decir, que la aparición de una palabra no depende de otra.  \n",
        "Aunque esta suposición no siempre es cierta, **el modelo funciona sorprendentemente bien** en la práctica.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🧠 Modelos a Probar**\n",
        "\n",
        "1. **`MultinomialNB`**  \n",
        "   - Es el **modelo clásico** de Naïve Bayes para texto.  \n",
        "   - Funciona muy bien con **conteos o frecuencias de palabras**.\n",
        "\n",
        "2. **`ComplementNB`**  \n",
        "   - Variante que mejora el rendimiento cuando el dataset está **desbalanceado**  \n",
        "     (algunas categorías tienen muchos más documentos que otras).  \n",
        "   - En lugar de calcular la probabilidad de una clase `C`, evalúa la probabilidad de **no pertenecer** a `C`,  \n",
        "     lo que **reduce el sesgo** hacia las clases más grandes.\n",
        "\n",
        "---\n",
        "\n",
        "#### **📊 Métrica de Evaluación: F1-Score (Macro)**\n",
        "\n",
        "La métrica **Accuracy (Exactitud)** no es la más adecuada en este caso,  \n",
        "ya que si una clase domina el conjunto de datos, un modelo puede obtener una  \n",
        "alta exactitud simplemente **prediciendo siempre esa clase**.\n",
        "\n",
        "Por eso se utiliza el **F1-Score**, que combina dos medidas:\n",
        "\n",
        "- **Precisión:** De las predicciones realizadas para una clase, ¿cuántas fueron correctas?  \n",
        "- **Recall:** De todos los ejemplos reales\n",
        "\n"
      ],
      "metadata": {
        "id": "3i69BPeuBbC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# --- Carga de Datos (Train y Test) ---\n",
        "# Ahora necesitamos ambos conjuntos: 'train' para entrenar el modelo y 'test' para evaluarlo\n",
        "# con datos que nunca ha visto.\n",
        "print(\"Cargando los conjuntos de entrenamiento y prueba...\")\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# --- Modelo Base (para tener un punto de comparación) ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Evaluando el Modelo Base (sin optimización)\")\n",
        "print(\"=\"*80)\n",
        "# Vectorizador simple, sin parámetros especiales.\n",
        "base_vect = TfidfVectorizer()\n",
        "# Aprende el vocabulario y transforma el conjunto de entrenamiento.\n",
        "X_train_base = base_vect.fit_transform(newsgroups_train.data)\n",
        "# Solo transforma el conjunto de prueba, usando el vocabulario ya aprendido.\n",
        "X_test_base = base_vect.transform(newsgroups_test.data)\n",
        "\n",
        "# Modelo Naive Bayes Multinomial con parámetros por defecto.\n",
        "base_clf = MultinomialNB()\n",
        "base_clf.fit(X_train_base, y_train) # Entrenamiento.\n",
        "y_pred_base = base_clf.predict(X_test_base) # Predicción.\n",
        "# Cálculo del F1-score macro para evaluar el rendimiento base.\n",
        "f1_base = f1_score(y_test, y_pred_base, average='macro')\n",
        "print(f\"F1-Score (Macro) del Modelo Base: {f1_base:.4f}\")\n",
        "print(f\"Tamaño del vocabulario del Modelo Base: {X_train_base.shape[1]} palabras\")\n",
        "\n",
        "# --- Resolución de la Consigna 3: Búsqueda del Mejor Modelo ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Optimizando Vectorizador y Modelos Naive Bayes\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Definir los parámetros optimizados para el vectorizador.\n",
        "vectorizer_params = {\n",
        "    'stop_words': 'english', # Eliminar palabras comunes.\n",
        "    'ngram_range': (1, 2),   # Considerar palabras individuales (unigramas) y pares de palabras (bigramas).\n",
        "    'min_df': 3,             # La palabra debe aparecer en al menos 3 documentos para ser considerada.\n",
        "    'max_df': 0.7            # Ignorar palabras que aparecen en más del 70% de los documentos.\n",
        "}\n",
        "\n",
        "# 2. Definir los modelos a probar con sus parámetros.\n",
        "#    'alpha' es un parámetro de suavizado. Un valor más bajo le da más confianza a los datos de entrenamiento.\n",
        "models_to_try = {\n",
        "    'MultinomialNB': MultinomialNB(alpha=0.1),\n",
        "    'ComplementNB': ComplementNB(alpha=0.5) # ComplementNB es ideal para datasets desbalanceados.\n",
        "}\n",
        "\n",
        "best_f1 = 0\n",
        "best_model_name = \"\"\n",
        "\n",
        "# 3. Instanciar y aplicar el vectorizador optimizado.\n",
        "print(\"Vectorizando los datos con parámetros optimizados...\")\n",
        "optimized_vect = TfidfVectorizer(**vectorizer_params)\n",
        "X_train_opt = optimized_vect.fit_transform(newsgroups_train.data)\n",
        "X_test_opt = optimized_vect.transform(newsgroups_test.data)\n",
        "# El tamaño del vocabulario cambia debido a los nuevos filtros (min_df, max_df, ngrams).\n",
        "print(f\"Nuevo tamaño del vocabulario optimizado: {X_train_opt.shape[1]} palabras\")\n",
        "\n",
        "# 4. Entrenar y evaluar cada modelo candidato.\n",
        "for model_name, model in models_to_try.items():\n",
        "    print(f\"\\n--- Entrenando y evaluando el modelo: {model_name} ---\")\n",
        "\n",
        "    # Entrenar el clasificador con los datos de entrenamiento optimizados.\n",
        "    model.fit(X_train_opt, y_train)\n",
        "\n",
        "    # Predecir las categorías en el conjunto de prueba.\n",
        "    y_pred = model.predict(X_test_opt)\n",
        "\n",
        "    # Calcular el F1-score macro para evaluar el rendimiento.\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    print(f\"F1-Score (Macro) para {model_name}: {f1:.4f}\")\n",
        "\n",
        "    # Guardar el mejor resultado encontrado hasta ahora.\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model_name = model_name\n",
        "\n",
        "# 5. Reportar el resultado final de la optimización.\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Resultados Finales de la Optimización\")\n",
        "print(\"=\"*80)\n",
        "print(f\"El mejor F1-Score (Macro) obtenido fue: {best_f1:.4f}\")\n",
        "print(f\"Este resultado fue logrado con el modelo: {best_model_name}\")\n",
        "print(f\"La mejora total respecto al F1-Score base ({f1_base:.4f}) es de: {best_f1 - f1_base:+.4f} puntos.\")"
      ],
      "metadata": {
        "id": "HLqhJ699XMGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b370a8-f81c-4c7d-e714-4bc677ebc119"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando los conjuntos de entrenamiento y prueba...\n",
            "\n",
            "================================================================================\n",
            "Evaluando el Modelo Base (sin optimización)\n",
            "================================================================================\n",
            "F1-Score (Macro) del Modelo Base: 0.5854\n",
            "Tamaño del vocabulario del Modelo Base: 101631 palabras\n",
            "\n",
            "================================================================================\n",
            "Optimizando Vectorizador y Modelos Naive Bayes\n",
            "================================================================================\n",
            "Vectorizando los datos con parámetros optimizados...\n",
            "Nuevo tamaño del vocabulario optimizado: 62739 palabras\n",
            "\n",
            "--- Entrenando y evaluando el modelo: MultinomialNB ---\n",
            "F1-Score (Macro) para MultinomialNB: 0.6802\n",
            "\n",
            "--- Entrenando y evaluando el modelo: ComplementNB ---\n",
            "F1-Score (Macro) para ComplementNB: 0.7026\n",
            "\n",
            "================================================================================\n",
            "Resultados Finales de la Optimización\n",
            "================================================================================\n",
            "El mejor F1-Score (Macro) obtenido fue: 0.7026\n",
            "Este resultado fue logrado con el modelo: ComplementNB\n",
            "La mejora total respecto al F1-Score base (0.5854) es de: +0.1172 puntos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observaciones finales**\n",
        "\n",
        "Al analizar y comparar los resultados, se puede concluir que la **optimización fue exitosa y significativa**.  \n",
        "Los ajustes aplicados al vectorizador y al clasificador lograron **mejorar notablemente el rendimiento** del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "#### **📉 Modelo Base**\n",
        "\n",
        "- El **F1-score** del modelo base ronda **0.58**, lo cual indica que,  \n",
        "  sin optimización, el modelo ya clasifica **mejor que al azar**,  \n",
        "  pero aún tiene **margen de mejora considerable**.  \n",
        "- El **vocabulario es muy grande**, lo que introduce **ruido** y disminuye la precisión,  \n",
        "  ya que incluye muchas palabras poco relevantes o redundantes.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🚀 Modelos Optimizados**\n",
        "\n",
        "##### **📚 Tamaño del Vocabulario**\n",
        "El vectorizador optimizado genera un **vocabulario más amplio**,  \n",
        "principalmente gracias al parámetro `ngram_range=(1, 2)`,  \n",
        "que incluye **palabras sueltas** y **pares de palabras** (*bigramas*).  \n",
        "\n",
        "Esto **enriquece las características lingüísticas** del modelo,  \n",
        "permitiéndole **capturar mejor el contexto** dentro del texto.\n",
        "\n",
        "##### **📈 Rendimiento**\n",
        "Los modelos optimizados, tanto **`MultinomialNB`** como **`ComplementNB`**,  \n",
        "**superan ampliamente al modelo base**.  \n",
        "\n",
        "El **F1-score** alcanza valores **cercanos o superiores a 0.70**,  \n",
        "representando una **mejora superior a 10 puntos porcentuales**,  \n",
        "lo cual es un **salto de calidad muy significativo** en Machine Learning.\n",
        "\n",
        "##### **🏆 Mejor Modelo**\n",
        "En la mayoría de los casos, **`ComplementNB`** obtiene un rendimiento **ligeramente superior** a `MultinomialNB`.  \n",
        "Esto confirma su **robustez en datasets desbalanceados**,  \n",
        "donde algunas categorías tienen muchos más ejemplos que otras.\n",
        "\n",
        "---\n",
        "\n",
        "#### **✅ Conclusión**\n",
        "\n",
        "Se cumplió exitosamente el **objetivo de la consigna**:  \n",
        "- Se demostró que **ajustando los parámetros de vectorización**  \n",
        "  y **seleccionando la variante adecuada del clasificador**,  \n",
        "  se puede **maximizar el desempeño del modelo Naïve Bayes**.\n",
        "\n",
        "El resultado es un **clasificador de texto mucho más preciso, equilibrado y confiable**,  \n",
        "capaz de **distinguir de manera efectiva entre las 20 categorías** del dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "RbQwWc5LBkan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **3.Referencias y Recursos de Inspiración**\n",
        "\n",
        "\n",
        "\n",
        "#### **📚 Librerías (Documentación Oficial)**\n",
        "\n",
        "La documentación oficial es siempre la fuente más **precisa y completa** para aprender y consultar detalles técnicos.\n",
        "\n",
        "- **Scikit-learn:**  \n",
        "  Librería central utilizada en el proyecto.\n",
        "\n",
        "- **Tutorial de trabajo con datos de texto:**  \n",
        "  Un tutorial oficial muy completo que usa el dataset *20 newsgroups* y explica paso a paso la **vectorización** y **clasificación** de texto.\n",
        "\n",
        "- **Documentación de `TfidfVectorizer`:**  \n",
        "  Explica en detalle los parámetros de la vectorización TF-IDF, como `ngram_range`, `min_df`, y otros ajustes de preprocesamiento.\n",
        "\n",
        "- **Documentación de `cosine_similarity`:**  \n",
        "  Describe el cálculo matemático detrás de la **similaridad del coseno**, usada para medir la relación entre textos o palabras.\n",
        "\n",
        "- **Documentación de `MultinomialNB` y `ComplementNB`:**  \n",
        "  Página oficial que explica los diferentes **clasificadores Naïve Bayes**, sus **hipótesis** y **casos de uso**.\n",
        "\n",
        "- **NumPy:**  \n",
        "  Librería fundamental para **cómputo numérico** y manipulación de arrays en Python.\n",
        "\n",
        "- **Documentación de `numpy.random.choice`:**  \n",
        "  Explica cómo funciona la **selección aleatoria de elementos**, usada para elegir documentos al azar.\n",
        "\n",
        "- **Documentación de `numpy.argsort`:**  \n",
        "  Describe cómo **ordenar índices** según valores, esencial para encontrar los textos o palabras más similares.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🧠 Artículos y Tutoriales**\n",
        "\n",
        "Estos recursos ofrecen una perspectiva más **conceptual** y ejemplos prácticos que complementan la documentación técnica.\n",
        "\n",
        "- **“TF-IDF for Machine Learning, Explained”** *(Towards Data Science)*  \n",
        "  Artículo que desglosa la **matemática y la intuición** detrás del método TF-IDF de forma clara y didáctica.\n",
        "\n",
        "- **“Naive Bayes Classification with Sklearn”** *(DataCamp)*  \n",
        "  Tutorial práctico sobre **clasificación de texto con Naïve Bayes**, muy similar a la consigna 3 del proyecto.\n",
        "\n",
        "- **“The Cosine Similarity Metric”** *(DeepAI)*  \n",
        "  Explicación concisa y visual de la **similaridad del coseno**, mostrando por qué es ideal para comparar textos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **🎥 Videos de YouTube**\n",
        "\n",
        "El formato de video es excelente para **visualizar conceptos complejos** de forma más intuitiva y amena.\n",
        "\n",
        "- **StatQuest: “Naive Bayes, Clearly Explained” — Josh Starmer**  \n",
        "  Video altamente recomendado que explica la **lógica de Naïve Bayes** con ejemplos simples.  \n",
        "  El canal *StatQuest* es una **referencia reconocida** para comprender algoritmos de Machine Learning.\n",
        "\n",
        "- **“Natural Language Processing (NLP) Tutorial with Python & NLTK” — Sentdex**  \n",
        "  Aunque utiliza la librería **NLTK**, es una excelente introducción a los fundamentos del PLN:  \n",
        "  *stop words, tokenización y vectorización de texto.*\n",
        "\n",
        "- **“Cosine Similarity, Clearly Explained” — Serrano.Academy**  \n",
        "  Video corto y claro que muestra la **intuición geométrica** detrás de la similaridad del coseno,  \n",
        "  ayudando a **visualizar por qué funciona** en la comparación de textos.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJw7Se0aR90O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}