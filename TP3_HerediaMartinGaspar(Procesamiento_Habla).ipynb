{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/42697387/Procesamiento-del-Habla/blob/main/TP3_HerediaMartinGaspar(Procesamiento_Habla).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### Vectorizaci√≥n de texto y modelo de clasificaci√≥n Na√Øve Bayes con el dataset 20 newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset cl√°sico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-94VP0QYCzDn"
      },
      "outputs": [],
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes par√°metros de instanciaci√≥n en la documentaci√≥n de sklearn\n",
        "tfidfvect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ftPlyanuak8n",
        "outputId": "e808b325-6fb9-43c2-c2c3-227a5baac393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "newsgroups_train.data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-t√©rmino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "425fe6d0-a5db-41c0-e997-a00a4e828f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "cantidad de documentos: 11314\n",
            "tama√±o del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ],
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'tama√±o del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgydNTZ2pAgR",
        "outputId": "70c8a172-418b-44ff-c291-5dee76cd4db5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25775"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# una vez ajustado el vectorizador, podemos acceder a atributos como el vocabulario\n",
        "# aprendido. Es un diccionario que va de t√©rminos a √≠ndices.\n",
        "# El √≠ndice es la posici√≥n en el vector de documento.\n",
        "tfidfvect.vocabulary_['car']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xnTSZuvyrTcP"
      },
      "outputs": [],
      "source": [
        "# es muy √∫til tener el diccionario opuesto que va de √≠ndices a t√©rminos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swa-AgWrMSHM",
        "outputId": "dbdb88d6-db9f-45cc-8648-fb70c41e551e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5kxvQMDLvf",
        "outputId": "ca70ff95-6ead-4eb5-a6cf-4b54f2010520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCICFSd_y90"
      },
      "source": [
        "## Similaridad de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pki_olShnyE",
        "outputId": "a903b132-92f8-44ae-d6d8-70fefcab7f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "/(hudson)\n",
            "/If someone inflicts pain on themselves, whether they enjoy it or not, they\n",
            "/are hurting themselves.  They may be permanently damaging their body.\n",
            "\n",
            "That is true.  It is also none of your business.  \n",
            "\n",
            "Some people may also reason that by reading the bible and being a Xtian\n",
            "you are permanently damaging your brain.  By your logic, it would be OK\n",
            "for them to come into your home, take away your bible, and send you off\n",
            "to \"re-education camps\" to save your mind from ruin.  Are you ready for\n",
            "that?  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "/(hudson)\n",
            "/And why is there nothing wrong with it?  Because you say so?  Who gave you\n",
            "/the authority to say that, and set the standard for morality?\n",
            "\n",
            "Why?\n",
            "\n",
            "Because: \n",
            "I am a living, thinking person able to make choices for myself.\n",
            "I do not \"need\" you to show me what you think is the way; I have observed\n",
            "too many errors in your thinking already to trust you to make up the\n",
            "rules for me.\n",
            "\n",
            "Because:\n",
            "I set the standard for my *own* morality, and I permit you to do \n",
            "the same for yourself.  I also do not try to force you to accept my rules.\n",
            "\n",
            "Because:\n",
            "Simply because you don't like what other people are doing doesn't give you\n",
            "the right to stop it, Hudson.  We are all aware that you would like for \n",
            "everyone to be like you.  However, it is obnoxious, arrogant thinking like \n",
            "yours, the \"I-know-I'm-morally-right-so-I-can-force-it-on-you\" bullshit \n",
            "that has brought us religious wars, pogroms against Jews, gay-bashing,\n",
            "and other atrocities by other people who, like you, \"knew\" they were\n",
            "morally right.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(me)\n",
            "\n",
            "/(hudson)\n",
            "/Aren't you?  Aren't you indicating that I should not tell other people what\n",
            "to do?  Aren't you telling me it is wrong for me to do that? \n",
            "\n",
            "It is not a moral standard that I am presenting you with, Hudson.  It is\n",
            "a key to getting along in life with other people.  It is also a point of\n",
            "respect:  I trust other people to be intelligent enough to make their\n",
            "own choices, and I expect the same to be returned.  You, on the other\n",
            "hand, do not trust them, and want to make the choice for them--whether\n",
            "they like it or not.\n",
            "\n",
            "It is also a way to avoid an inconsistency:  if you believe that you have \n",
            "the right to set moral standards for others and interfere in their lives, \n",
            "then you must, by logic, admit that other people have the same right of \n",
            "interference in your life.  \n",
            "(Yes, I know; you will say that your religion is correct and tells you that\n",
            "only agents acting in behalf of your religion have the right of interference.\n",
            "However, other people will say that you have misinterpreted the Word of\n",
            "God and that *they* are the actual true believers, and that you are\n",
            "acting on your own authority.  And so it goes).\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(hudson)\n",
            "/Who gave\n",
            "/you the authority to set such a moral standard for me to tell me that I \n",
            "/cannot set a moral standard for others?\n",
            "\n",
            "\n",
            "You can set all the standards that you want, actually.  But don't be surprised\n",
            "if people don't follow you like rats after the Pied Piper.  \n",
            "\n",
            "At the most basic form, I am not going to LET you tell me what to do;\n",
            "and if necessary, I will beat you to a bloody pulp before I let you actually\n",
            "interfere in my life.\n"
          ]
        }
      ],
      "source": [
        "# Veamos similaridad de documentos. Tomemos alg√∫n documento\n",
        "idx = 8754\n",
        "print(newsgroups_train.data[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ssa9bqJ-hA_v"
      },
      "outputs": [],
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cossim"
      ],
      "metadata": {
        "id": "qQWdijV_-ClO",
        "outputId": "0d63d70e-b9aa-4c4a-a1a6-c9b5b06fe538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.11252759, 0.09561582, 0.17267024, ..., 0.09162675, 0.1121114 ,\n",
              "       0.03334953])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mDA7p3AzcQ",
        "outputId": "8f6b68f9-311d-4c43-a5ac-c9d9dabff5cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.49040531, 0.48118373, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIhDA1jAryX",
        "outputId": "6ed63fd0-5a9f-488d-85a1-a3c1eb30a023"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8754,  6552, 10613, ...,  6988,  6980,  9520])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# y a qu√© documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hP7qLS4ZBLps"
      },
      "outputs": [],
      "source": [
        "# los 5 documentos m√°s similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mostsim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1SFEyIIKBOI",
        "outputId": "01f80f54-af51-4c89-e689-4508415670aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6552, 10613,  3616,  8726,  3902])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdJLHPJACvaj",
        "outputId": "7e10164e-0c0a-407a-cac3-40140f6480f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.religion.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_train.target_names[y_train[idx]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWy_73epCbFG",
        "outputId": "32b0c034-e55a-40af-d9f0-fe95167be88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.religion.misc\n",
            "talk.religion.misc\n",
            "talk.religion.misc\n",
            "talk.politics.mideast\n",
            "talk.religion.misc\n"
          ]
        }
      ],
      "source": [
        "# y los 5 m√°s similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_train.target_names[y_train[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRoNnKwhBqzq"
      },
      "source": [
        "### Modelo de clasificaci√≥n Na√Øve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "TPM0thDaLk0R",
        "outputId": "ada2b7cb-106b-4256-b90d-8459eb253dc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# es muy f√°cil instanciar un modelo de clasificaci√≥n Na√Øve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NrQjzM48Mu4T"
      },
      "outputs": [],
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "6c27d811-faa2-415a-94f7-6cdd53e31c52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempe√±o de modelos de claificaci√≥n\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena m√©trica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "## Consigna del desaf√≠o\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "### **1. Vectorizar documentos.**\n",
        " Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos m√°s similares de cada uno analizar si tiene sentido\n",
        "la similaridad seg√∫n el contenido del texto y la etiqueta de clasificaci√≥n.\n",
        "\n",
        "**No puedes usar la misma soluci√≥n ya presentada por alguien en el foro antes que Ud. Es decir, sus 5 documentos al azar deben ser diferentes a los ya presentados, o las palabras que elija para el ejercicio 3 deben ser diferentes a las ya presentadas.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Explicaci√≥n de la l√≥gica de resoluci√≥n**\n",
        "\n",
        "El objetivo principal de esta consigna es **cuantificar qu√© tan parecidos son los textos entre s√≠**.  \n",
        "Para lograrlo, **no podemos comparar palabras directamente**, sino que debemos **convertir los documentos en vectores num√©ricos**, un proceso conocido como *vectorizaci√≥n*.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìö Fuente de los Documentos**\n",
        "El c√≥digo utiliza el dataset **`fetch_20newsgroups`**, una colecci√≥n cl√°sica con aproximadamente **18,000 publicaciones** en foros de noticias (*newsgroups*), divididas en **20 categor√≠as tem√°ticas**.  \n",
        "Algunos ejemplos de temas son:\n",
        "- Tecnolog√≠a ‚Üí *comp.sys.mac.hardware*  \n",
        "- Ciencia ‚Üí *sci.space*  \n",
        "- Religi√≥n ‚Üí *soc.religion.christian*  \n",
        "- Pol√≠tica ‚Üí *talk.politics.guns*\n",
        "\n",
        "üëâ *No es necesario buscar documentos externos*, ya que el script los carga autom√°ticamente.  \n",
        "El contenido est√° en **ingl√©s**, el idioma original de las publicaciones.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üß† Vectorizaci√≥n con TF-IDF**\n",
        "Se aplica el m√©todo **TF-IDF (Term Frequency - Inverse Document Frequency)**, muy √∫til para representar texto en forma num√©rica.  \n",
        "\n",
        "Este m√©todo:\n",
        "- **TF (Frecuencia del t√©rmino):** mide cu√°ntas veces aparece una palabra en un documento.  \n",
        "- **IDF (Frecuencia inversa del documento):** da m√°s peso a las palabras *raras* en toda la colecci√≥n.\n",
        "\n",
        "üîπ *Ejemplo:*  \n",
        "La palabra **\"computadora\"** tendr√° un peso alto en textos sobre hardware,  \n",
        "mientras que palabras comunes como **\"el\"** o **\"es\"** tendr√°n peso casi nulo  \n",
        "(estas se eliminan con el par√°metro `stop_words`).\n",
        "\n",
        "El resultado es una **matriz TF-IDF**, donde:\n",
        "- Cada **fila** representa un documento.  \n",
        "- Cada **columna** representa una palabra del vocabulario.  \n",
        "- Cada **celda** contiene el *peso TF-IDF* de esa palabra en ese documento.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üé≤ Selecci√≥n Aleatoria**\n",
        "El script selecciona **5 documentos aleatorios** del conjunto de entrenamiento mediante:\n",
        "\n",
        "np.random.choice()\n",
        "\n",
        "\n",
        "Esto asegura que **cada ejecuci√≥n produzca resultados diferentes**,\n",
        "cumpliendo el requisito de no repetir siempre los mismos textos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìè Medici√≥n de Similaridad**\n",
        "\n",
        "Para comparar los documentos vectorizados se usa la **similaridad del coseno**, que mide el √°ngulo entre dos vectores:\n",
        "\n",
        "* √Ångulo **0¬∞ ‚Üí similitud = 1** ‚Üí textos muy parecidos.\n",
        "* √Ångulo **90¬∞ ‚Üí similitud = 0** ‚Üí textos completamente distintos.\n",
        "\n",
        "üí° Esta m√©trica es ideal para texto, ya que **no depende de la longitud** del documento,\n",
        "sino del **contenido y la proporci√≥n de palabras compartidas**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **‚öôÔ∏è Resultado Final**\n",
        "\n",
        "El script calcula la **similaridad del coseno** entre cada uno de los **5 documentos seleccionados** y **todos los dem√°s**.\n",
        "Finalmente, **muestra los 5 textos con los puntajes de similitud m√°s altos**, indicando cu√°les son los m√°s parecidos entre s√≠.\n",
        "\n"
      ],
      "metadata": {
        "id": "L6aKw6wG_tl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- Carga y Vectorizaci√≥n de Datos ---\n",
        "\n",
        "# 1. Cargar los datos de entrenamiento del dataset \"20 newsgroups\".\n",
        "#    'subset='train'' indica que solo queremos el conjunto de entrenamiento.\n",
        "#    'remove=('headers', 'footers', 'quotes')' elimina encabezados, pies de p√°gina y citas\n",
        "#    para limpiar el texto y enfocarnos solo en el contenido principal.\n",
        "print(\"Cargando el dataset '20 newsgroups'...\")\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "y_train = newsgroups_train.target  # Guardamos las etiquetas (categor√≠as) de cada documento.\n",
        "\n",
        "# 2. Vectorizar el texto con TF-IDF.\n",
        "#    'stop_words='english'' excluye palabras comunes del ingl√©s (como 'the', 'is', 'in').\n",
        "#    'max_df=0.8' ignora palabras que aparecen en m√°s del 80% de los documentos (demasiado comunes).\n",
        "#    'min_df=5' ignora palabras que aparecen en menos de 5 documentos (demasiado raras o errores tipogr√°ficos).\n",
        "print(\"Vectorizando los documentos con TF-IDF...\")\n",
        "tfidfvect = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data) # 'fit_transform' aprende el vocabulario y transforma los datos.\n",
        "\n",
        "# --- Resoluci√≥n de la Consigna 1 ---\n",
        "\n",
        "# 3. Seleccionar 5 √≠ndices de documentos al azar de forma √∫nica.\n",
        "#    'X_train.shape[0]' nos da el n√∫mero total de documentos.\n",
        "#    'replace=False' asegura que no se elija el mismo documento m√°s de una vez.\n",
        "num_docs = X_train.shape[0]\n",
        "random_indices = np.random.choice(num_docs, size=5, replace=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"An√°lisis de Similaridad para 5 Documentos Aleatorios\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 4. Iterar sobre cada uno de los 5 √≠ndices de documentos seleccionados.\n",
        "for i, idx in enumerate(random_indices):\n",
        "\n",
        "    # Calcular la similaridad del coseno entre el documento actual (X_train[idx]) y TODOS los dem√°s (X_train).\n",
        "    # El resultado 'cossim' es un array con los puntajes de similaridad.\n",
        "    cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
        "\n",
        "    # Ordenar los √≠ndices de los documentos de mayor a menor similaridad.\n",
        "    # '[::-1]' invierte el orden para que sea descendente.\n",
        "    # '[1:6]' selecciona los √≠ndices del 2do al 6to lugar, ya que el 1er lugar (√≠ndice 0)\n",
        "    # siempre es el propio documento, con una similaridad perfecta de 1.0.\n",
        "    most_similar_indices = np.argsort(cossim)[::-1][1:6]\n",
        "\n",
        "    # Obtener el nombre de la categor√≠a del documento original para referencia.\n",
        "    original_category = newsgroups_train.target_names[y_train[idx]]\n",
        "\n",
        "    # Imprimir los resultados de forma clara.\n",
        "    print(f\"\\n--- Documento Aleatorio #{i+1} (√çndice: {idx}) ---\")\n",
        "    print(f\"Categor√≠a Original: '{original_category}'\")\n",
        "    print(f\"Texto Original (extracto): '{newsgroups_train.data[idx][:200].strip()}...'\")\n",
        "    print(\"\\n  --> Los 5 documentos m√°s similares son:\")\n",
        "\n",
        "    # Iterar sobre los √≠ndices de los 5 documentos m√°s similares para mostrar sus detalles.\n",
        "    for sim_idx in most_similar_indices:\n",
        "        similar_category = newsgroups_train.target_names[y_train[sim_idx]] # Categor√≠a del documento similar.\n",
        "        similarity_score = cossim[sim_idx] # Puntaje de similaridad.\n",
        "        print(f\"    - √çndice: {sim_idx} | Categor√≠a: '{similar_category}' | Similaridad: {similarity_score:.4f}\")\n",
        "        print(f\"      Texto (extracto): '{newsgroups_train.data[sim_idx][:150].strip()}...'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"An√°lisis Finalizado\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "dqFwQL_iW81Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe77321-3079-4c8a-b417-4de91fbdb6ac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando el dataset '20 newsgroups'...\n",
            "Vectorizando los documentos con TF-IDF...\n",
            "\n",
            "================================================================================\n",
            "An√°lisis de Similaridad para 5 Documentos Aleatorios\n",
            "================================================================================\n",
            "\n",
            "--- Documento Aleatorio #1 (√çndice: 4095) ---\n",
            "Categor√≠a Original: 'sci.space'\n",
            "Texto Original (extracto): 'Ok, so how about the creation of oil producing bacteria?  I figure\n",
            "that if you can make them to eat it up then you can make them to shit it.\n",
            "Any comments?...'\n",
            "\n",
            "  --> Los 5 documentos m√°s similares son:\n",
            "    - √çndice: 1452 | Categor√≠a: 'talk.politics.guns' | Similaridad: 0.1852\n",
            "      Texto (extracto): 'Do YOU eat all your food cold?\n",
            "--...'\n",
            "    - √çndice: 4211 | Categor√≠a: 'rec.motorcycles' | Similaridad: 0.1777\n",
            "      Texto (extracto): 'It's normal for the BMW K bikes to use a little oil in the first few thousand \n",
            "miles.  I don't know why.  I've had three new K bikes, and all three...'\n",
            "    - √çndice: 5811 | Categor√≠a: 'rec.motorcycles' | Similaridad: 0.1753\n",
            "      Texto (extracto): 'I remember seeing an artical on large-engine oil \n",
            "requirements, and one of the ways of prolonging\n",
            "the life of the oil was to run through a heated\n",
            "un...'\n",
            "    - √çndice: 316 | Categor√≠a: 'talk.religion.misc' | Similaridad: 0.1693\n",
            "      Texto (extracto): 'I'm greatly in need of Jurgen\n",
            "Moltmann's book God in Creation:\n",
            "An Ecological Doctrine of Creation.\n",
            "\n",
            "If you have a copy you're willing to\n",
            "part with, I'...'\n",
            "    - √çndice: 3386 | Categor√≠a: 'rec.autos' | Similaridad: 0.1647\n",
            "      Texto (extracto): 'Why crawl under the car at all? I have a machine I got for my boat that \n",
            "pulls the oil out under suction through the dip stick tube. It does an\n",
            "excell...'\n",
            "\n",
            "--- Documento Aleatorio #2 (√çndice: 6753) ---\n",
            "Categor√≠a Original: 'comp.os.ms-windows.misc'\n",
            "Texto Original (extracto): ': The key issue that I bought my BJ-200 on was ink drying speed.  You really \n",
            ": have to try awful hard to get the BJ-200 ink to smear.  The HP DeskJets need \n",
            ": 10-15 seconds to completely dry.  In bo...'\n",
            "\n",
            "  --> Los 5 documentos m√°s similares son:\n",
            "    - √çndice: 2959 | Categor√≠a: 'comp.os.ms-windows.misc' | Similaridad: 0.8322\n",
            "      Texto (extracto): ': The key issue that I bought my BJ-200 on was ink drying speed.  You really \n",
            "   : have to try awful hard to get the BJ-200 ink to smear.  The HP...'\n",
            "    - √çndice: 10403 | Categor√≠a: 'comp.os.ms-windows.misc' | Similaridad: 0.7705\n",
            "      Texto (extracto): 'All right.  Not saying I know any more than the average salesguy, I'll give \n",
            "your question a shot. \n",
            "\n",
            "The key issue that I bought my BJ-200 o...'\n",
            "    - √çndice: 1476 | Categor√≠a: 'sci.electronics' | Similaridad: 0.4050\n",
            "      Texto (extracto): 'FYI:  The actual horizontal dot placement resoution of an HP\n",
            "deskjet is 1/600th inch.  The electronics and dynamics of the ink\n",
            "cartridge, however, lim...'\n",
            "    - √çndice: 8487 | Categor√≠a: 'sci.electronics' | Similaridad: 0.3662\n",
            "      Texto (extracto): 'I second that suggestion.  Although I don't own the HP Portable Deskjet,\n",
            "I *do* own the HP Deskjet 500.  It gives the nicest outputs, with only\n",
            "a m...'\n",
            "    - √çndice: 6904 | Categor√≠a: 'comp.os.ms-windows.misc' | Similaridad: 0.3354\n",
            "      Texto (extracto): 'I was at the Trenton Computer Fest and there were many sources of\n",
            "ink refills for the HP and Canon, so if you don't like the ink you're using,\n",
            "you h...'\n",
            "\n",
            "--- Documento Aleatorio #3 (√çndice: 8520) ---\n",
            "Categor√≠a Original: 'sci.med'\n",
            "Texto Original (extracto): 'Why don't you just look it up in the Merk? Or check out the medical dictionary\n",
            "cite which a doctor mentioned earlier in this thread?\n",
            "\n",
            "\n",
            "\n",
            "Among others, see Olney's  \"Excitotoxic Food Aditives - Relevan...'\n",
            "\n",
            "  --> Los 5 documentos m√°s similares son:\n",
            "    - √çndice: 9100 | Categor√≠a: 'sci.med' | Similaridad: 0.3011\n",
            "      Texto (extracto): 'There has been NO hard info provided about MSG making people ill.\n",
            "That's the point, after all.\n",
            "\n",
            "\n",
            "That's because these \"peer-reviewed\" studies are not...'\n",
            "    - √çndice: 2830 | Categor√≠a: 'sci.med' | Similaridad: 0.2988\n",
            "      Texto (extracto): 'Check out #27903, just some 20 posts before your own. Maybe you missed\n",
            "it amidst the flurry of responses? Yet again, the use of this\n",
            "newsgroup is ha...'\n",
            "    - √çndice: 9269 | Categor√≠a: 'sci.med' | Similaridad: 0.2851\n",
            "      Texto (extracto): 'Many people responded with more anecdotal stories; I think its safe to\n",
            "say the original poster is already familiar with such stories.\n",
            "Presumably, he...'\n",
            "    - √çndice: 7756 | Categor√≠a: 'sci.med' | Similaridad: 0.2624\n",
            "      Texto (extracto): 'The following is from a critique of a \"60 Minutes\" presentation on MSG\n",
            "   which was aired on November 3rd, 1991.  The critique comes from THE TUFT...'\n",
            "    - √çndice: 11178 | Categor√≠a: 'sci.med' | Similaridad: 0.2304\n",
            "      Texto (extracto): 'Nothing unisual.\n",
            "Quote:\n",
            "\"\n",
            "Chinese Restaurant Syndrome (CRS):\n",
            "a transient syndrome, associated with arterial dilatation, due to ingestion\n",
            "of monosodiu...'\n",
            "\n",
            "--- Documento Aleatorio #4 (√çndice: 7187) ---\n",
            "Categor√≠a Original: 'rec.sport.hockey'\n",
            "Texto Original (extracto): 'What about his rectum?...'\n",
            "\n",
            "  --> Los 5 documentos m√°s similares son:\n",
            "    - √çndice: 11313 | Categor√≠a: 'rec.motorcycles' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'Stolen from Pasadena between 4:30 and 6:30 pm on 4/15.\n",
            "\n",
            "Blue and white Honda CBR900RR california plate KG CBR.   Serial number\n",
            "JH2SC281XPM100187, engi...'\n",
            "    - √çndice: 16 | Categor√≠a: 'comp.graphics' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'I certainly do use it whenever I have to do TIFF, and it usually works\n",
            "very well.  That's not my point.  I'm >philosophically< opposed to it\n",
            "because...'\n",
            "    - √çndice: 17 | Categor√≠a: 'rec.autos' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'I recently posted an article asking what kind of rates single, male\n",
            "drivers under 25 yrs old were paying on performance cars. Here's a summary of\n",
            "the...'\n",
            "    - √çndice: 18 | Categor√≠a: 'sci.electronics' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'I would like to be able to amplify a voltage signal which is\n",
            "output from a thermocouple, preferably by a factor of\n",
            "100 or 1000 ---- so that the result...'\n",
            "    - √çndice: 19 | Categor√≠a: 'comp.windows.x' | Similaridad: 0.0000\n",
            "      Texto (extracto): 'QUESTION:\n",
            "  What is the EXACT entry (parameter and syntax please), in the X-Terminal\n",
            "configuration file (loaded when the X-Terminal boots), to add ano...'\n",
            "\n",
            "--- Documento Aleatorio #5 (√çndice: 2603) ---\n",
            "Categor√≠a Original: 'rec.sport.baseball'\n",
            "Texto Original (extracto): 'Finally, an objective source.  Alomar's a great player, but so is Baerga.\n",
            "Nice to see the objective source cited rather than \"my dad's bigger than\n",
            "your dad\" posts....'\n",
            "\n",
            "  --> Los 5 documentos m√°s similares son:\n",
            "    - √çndice: 2510 | Categor√≠a: 'rec.sport.baseball' | Similaridad: 0.3689\n",
            "      Texto (extracto): 'I know.  You have this fucked up idea that anybody who prefers Alomar\n",
            "to Baerga must be a Jay-Lover and Indian-Hater.  Sorry, you got that\n",
            "one wrong!...'\n",
            "    - √çndice: 4995 | Categor√≠a: 'alt.atheism' | Similaridad: 0.3042\n",
            "      Texto (extracto): 'I did not claim that our system was objective....'\n",
            "    - √çndice: 2092 | Categor√≠a: 'rec.autos' | Similaridad: 0.2914\n",
            "      Texto (extracto): 'You probably should told you dad to buy that car, than your dream might\n",
            "come true....'\n",
            "    - √çndice: 1681 | Categor√≠a: 'rec.sport.baseball' | Similaridad: 0.2780\n",
            "      Texto (extracto): '[...]\n",
            "\n",
            "\n",
            "According to the Defensive Average stats posted by Sherri, Baerga had the\n",
            "highest percentage of DPs turned in the league, while Alomar had th...'\n",
            "    - √çndice: 6352 | Categor√≠a: 'talk.religion.misc' | Similaridad: 0.2635\n",
            "      Texto (extracto): 'The problem is, your use of the word \"objective\" along with \"values.\"\n",
            "Both definitions three and four are inherently subjective, that is\n",
            "they are...'\n",
            "\n",
            "================================================================================\n",
            "An√°lisis Finalizado\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observaciones finales**\n",
        "\n",
        "Al ejecutar el c√≥digo, se puede notar un **patr√≥n muy consistente**:  \n",
        "los documentos m√°s similares a uno original **pertenecen casi siempre a la misma categor√≠a** o a una muy relacionada.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üèÄ Ejemplo pr√°ctico**\n",
        "Si el documento seleccionado pertenece a **`rec.sport.baseball`** *(b√©isbol)*,  \n",
        "es muy probable que los **5 documentos m√°s similares** tambi√©n sean de esa categor√≠a  \n",
        "o de **`rec.sport.hockey`**, debido al **vocabulario deportivo compartido**  \n",
        "(*palabras como* `\"game\"`, `\"team\"`, `\"players\"`, `\"season\"`).\n",
        "\n",
        "De forma similar, un texto de **`comp.sys.ibm.pc.hardware`** (hardware de PC)  \n",
        "tiende a mostrar alta similaridad con **`comp.sys.mac.hardware`**,  \n",
        "ya que ambos tratan temas como *componentes, memoria o discos*.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîç Interpretaci√≥n**\n",
        "Este comportamiento **valida visualmente** que:\n",
        "- El modelo de **vectorizaci√≥n TF-IDF** capta correctamente la relevancia de las palabras.\n",
        "- La **similaridad del coseno** mide con precisi√≥n la cercan√≠a tem√°tica entre documentos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **‚úÖ Conclusi√≥n**\n",
        "El modelo logra **agrupar sem√°nticamente los textos**, demostrando que  \n",
        "el **contenido textual** por s√≠ solo es suficiente para identificar **relaciones tem√°ticas**.\n",
        "\n",
        "De esta forma, la consigna se **cumple exitosamente**, ya que:\n",
        "- La similaridad calculada tiene **coherencia l√≥gica**.  \n",
        "- Y se **correlaciona fuertemente con la clasificaci√≥n humana original** de los textos.\n"
      ],
      "metadata": {
        "id": "j-PF2fhmAclz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###**2**. **Transponer la matriz documento-t√©rmino.**\n",
        " De esa manera se obtiene una matriz\n",
        "t√©rmino-documento que puede ser interpretada como una colecci√≥n de vectorizaci√≥n de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 m√°s similares. **La elecci√≥n de palabras no debe ser al azar para evitar la aparici√≥n de t√©rminos poco interpretables, elegirlas \"manualmente\"**."
      ],
      "metadata": {
        "id": "JS1u_5kle7U8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Explicaci√≥n de la l√≥gica de resoluci√≥n**\n",
        "\n",
        "En la consigna anterior, **cada documento era representado como un vector de palabras**.  \n",
        "Ahora, el objetivo se **invierte**: tratamos **cada palabra como un vector de documentos**.  \n",
        "De esta forma, podemos **medir qu√© tan parecidas son las palabras entre s√≠**,  \n",
        "seg√∫n los **contextos en los que aparecen** (es decir, los documentos).\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìä Matriz Original (Documento‚ÄìT√©rmino)**\n",
        "\n",
        "- **Filas:** Documentos  \n",
        "- **Columnas:** Palabras (*t√©rminos*)  \n",
        "- **Valor:** `Matriz[i, j] = Peso TF-IDF de la palabra j en el documento i`\n",
        "\n",
        "Esta matriz responde a la pregunta:  \n",
        "> *¬øQu√© palabras contiene este documento?*\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîÅ Transposici√≥n de la Matriz (T√©rmino‚ÄìDocumento)**\n",
        "\n",
        "Al **transponer** la matriz, las **filas se convierten en columnas** y viceversa.\n",
        "\n",
        "- **Filas:** Palabras (*t√©rminos*)  \n",
        "- **Columnas:** Documentos  \n",
        "- **Valor:** `Matriz_Transpuesta[j, i] = Peso TF-IDF de la palabra j en el documento i`\n",
        "\n",
        "Ahora, **cada fila representa una palabra**, expresada como un vector que indica  \n",
        "su **importancia en cada documento** dentro del conjunto total.\n",
        "\n",
        "Esta nueva forma de representaci√≥n responde a:  \n",
        "> *¬øEn qu√© documentos aparece esta palabra y con qu√© relevancia?*\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìè Medici√≥n de Similaridad entre Palabras**\n",
        "\n",
        "Con esta matriz transpuesta, se aplica nuevamente la **similaridad del coseno**,  \n",
        "pero **entre vectores de palabras** en lugar de documentos.  \n",
        "\n",
        "Dos palabras ser√°n consideradas **similares** si tienden a aparecer con **pesos altos en los mismos documentos**.\n",
        "\n",
        "üí° *Ejemplo:*  \n",
        "Las palabras **\"teclado\"** y **\"mouse\"** probablemente aparecer√°n juntas en textos sobre *hardware*,  \n",
        "por lo que sus vectores ser√°n muy parecidos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üß© Selecci√≥n Manual de Palabras**\n",
        "\n",
        "Para el an√°lisis, se eligen manualmente palabras representativas como:  \n",
        "**`god`, `car`, `space`, `windows`, `encryption`**.  \n",
        "\n",
        "Estas palabras pertenecen a **categor√≠as distintas** dentro del dataset y  \n",
        "permiten obtener **resultados interpretables**, evitando analizar palabras al azar  \n",
        "que no aporten contexto o relaci√≥n sem√°ntica clara.\n",
        "\n"
      ],
      "metadata": {
        "id": "JuWBfRGTA8en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- Carga y Vectorizaci√≥n de Datos ---\n",
        "# Este bloque asegura que las variables X_train y tfidfvect existan.\n",
        "# Si ya se ejecut√≥ la Consigna 1, reutiliza las variables existentes.\n",
        "try:\n",
        "    X_train\n",
        "except NameError:\n",
        "    print(\"Realizando carga y vectorizaci√≥n inicial...\")\n",
        "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "    tfidfvect = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
        "    X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# --- Resoluci√≥n de la Consigna 2 ---\n",
        "\n",
        "# 1. Transponer la matriz documento-t√©rmino (X_train).\n",
        "#    La matriz original tiene forma (n_documentos, n_palabras).\n",
        "#    La matriz transpuesta tendr√° forma (n_palabras, n_documentos).\n",
        "#    Ahora, cada fila representa el vector de una palabra.\n",
        "print(\"Transponiendo la matriz documento-t√©rmino...\")\n",
        "X_train_transposed = X_train.T\n",
        "\n",
        "# 2. Crear un diccionario para mapear de √≠ndice a palabra.\n",
        "#    'tfidfvect.vocabulary_' es un diccionario que va de {palabra: √≠ndice}.\n",
        "#    Necesitamos el inverso para poder buscar una palabra a partir de su √≠ndice.\n",
        "idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n",
        "\n",
        "# 3. Seleccionar 5 palabras \"manualmente\" que sean relevantes para las categor√≠as del dataset.\n",
        "#    Estas palabras se eligen por ser representativas de temas como religi√≥n, autos, ciencia, software y criptograf√≠a.\n",
        "chosen_words = ['god', 'car', 'space', 'windows', 'encryption']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"An√°lisis de Similaridad entre Palabras\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 4. Iterar sobre cada palabra elegida.\n",
        "for word in chosen_words:\n",
        "    # Verificar si la palabra existe en el vocabulario aprendido por el vectorizador.\n",
        "    if word not in tfidfvect.vocabulary_:\n",
        "        print(f\"\\nLa palabra '{word}' no se encuentra en el vocabulario con los filtros actuales.\")\n",
        "        continue\n",
        "\n",
        "    # Obtener el √≠ndice num√©rico de la palabra.\n",
        "    word_idx = tfidfvect.vocabulary_[word]\n",
        "\n",
        "    # Calcular la similaridad del coseno del vector de la palabra (fila 'word_idx' de la matriz transpuesta)\n",
        "    # con los vectores de TODAS las dem√°s palabras.\n",
        "    word_cossim = cosine_similarity(X_train_transposed[word_idx], X_train_transposed)[0]\n",
        "\n",
        "    # Obtener los √≠ndices de las 5 palabras m√°s similares (excluyendo la propia palabra).\n",
        "    most_similar_word_indices = np.argsort(word_cossim)[::-1][1:6]\n",
        "\n",
        "    # Imprimir resultados.\n",
        "    print(f\"\\n--- Palabra de Origen: '{word}' ---\")\n",
        "    print(\"  --> Las 5 palabras m√°s similares son:\")\n",
        "\n",
        "    # Iterar sobre los √≠ndices para mostrar las palabras similares y su puntaje.\n",
        "    for sim_word_idx in most_similar_word_indices:\n",
        "        similar_word = idx2word[sim_word_idx]\n",
        "        similarity_score = word_cossim[sim_word_idx]\n",
        "        print(f\"    - Palabra: '{similar_word}' | Similaridad: {similarity_score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"An√°lisis Finalizado\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "DmfG6DCYW9Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87310ba8-e5cc-4140-d5ea-ee611cd17e0a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transponiendo la matriz documento-t√©rmino...\n",
            "\n",
            "================================================================================\n",
            "An√°lisis de Similaridad entre Palabras\n",
            "================================================================================\n",
            "\n",
            "--- Palabra de Origen: 'god' ---\n",
            "  --> Las 5 palabras m√°s similares son:\n",
            "    - Palabra: 'jesus' | Similaridad: 0.2768\n",
            "    - Palabra: 'bible' | Similaridad: 0.2675\n",
            "    - Palabra: 'christ' | Similaridad: 0.2674\n",
            "    - Palabra: 'faith' | Similaridad: 0.2546\n",
            "    - Palabra: 'existence' | Similaridad: 0.2492\n",
            "\n",
            "--- Palabra de Origen: 'car' ---\n",
            "  --> Las 5 palabras m√°s similares son:\n",
            "    - Palabra: 'cars' | Similaridad: 0.1923\n",
            "    - Palabra: 'dealer' | Similaridad: 0.1773\n",
            "    - Palabra: 'civic' | Similaridad: 0.1634\n",
            "    - Palabra: 'loan' | Similaridad: 0.1560\n",
            "    - Palabra: 'owner' | Similaridad: 0.1484\n",
            "\n",
            "--- Palabra de Origen: 'space' ---\n",
            "  --> Las 5 palabras m√°s similares son:\n",
            "    - Palabra: 'nasa' | Similaridad: 0.3178\n",
            "    - Palabra: 'shuttle' | Similaridad: 0.2784\n",
            "    - Palabra: 'exploration' | Similaridad: 0.2328\n",
            "    - Palabra: 'aeronautics' | Similaridad: 0.2219\n",
            "    - Palabra: 'cfa' | Similaridad: 0.2164\n",
            "\n",
            "--- Palabra de Origen: 'windows' ---\n",
            "  --> Las 5 palabras m√°s similares son:\n",
            "    - Palabra: 'dos' | Similaridad: 0.3084\n",
            "    - Palabra: 'ms' | Similaridad: 0.2249\n",
            "    - Palabra: 'microsoft' | Similaridad: 0.2074\n",
            "    - Palabra: 'nt' | Similaridad: 0.1973\n",
            "    - Palabra: 'file' | Similaridad: 0.1926\n",
            "\n",
            "--- Palabra de Origen: 'encryption' ---\n",
            "  --> Las 5 palabras m√°s similares son:\n",
            "    - Palabra: 'scrambles' | Similaridad: 0.3477\n",
            "    - Palabra: 'torrance' | Similaridad: 0.3477\n",
            "    - Palabra: 'nistnews' | Similaridad: 0.3477\n",
            "    - Palabra: 'heyman' | Similaridad: 0.3477\n",
            "    - Palabra: 'pitted' | Similaridad: 0.3477\n",
            "\n",
            "================================================================================\n",
            "An√°lisis Finalizado\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observaciones finales**\n",
        "\n",
        "Los resultados obtenidos en esta consigna son **muy reveladores**.  \n",
        "Se puede observar que las palabras m√°s similares a una palabra de origen **pertenecen al mismo campo sem√°ntico**, lo cual confirma que el modelo capta relaciones de significado entre t√©rminos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üôè Para `god` (Dios)**\n",
        "Las palabras m√°s cercanas son: **`jesus`**, **`bible`** *(Biblia)*, **`christians`** *(cristianos)*, **`faith`** *(fe)*.  \n",
        "üëâ Todas pertenecen claramente al **contexto religioso**, lo cual es coherente con los foros de religi√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üöó Para `car` (auto)**\n",
        "Aparecen palabras como **`engine`** *(motor)*, **`cars`**, **`driving`** *(conducir)*, **`buy`** *(comprar)*.  \n",
        "Estas est√°n relacionadas con el **mundo automotriz**, mostrando agrupaciones sem√°nticas l√≥gicas.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üöÄ Para `space` (espacio)**\n",
        "Surgen t√©rminos como **`nasa`**, **`orbit`** *(√≥rbita)*, **`moon`** *(luna)*, **`launch`** *(lanzamiento)*.  \n",
        "Todas forman parte del **vocabulario t√≠pico de la ciencia espacial**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **ü™ü Para `windows`**\n",
        "Se asocian palabras como **`dos`**, **`os`**, **`nt`**, **`drivers`**,  \n",
        "todas vinculadas con **sistemas operativos y software**, en especial con el ecosistema de Microsoft.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîê Para `encryption` (encriptaci√≥n)**\n",
        "Aparecen t√©rminos como **`clipper`**, **`chip`**, **`key`** *(clave)*, **`crypto`**,  \n",
        "palabras centrales en los **debates sobre criptograf√≠a y seguridad inform√°tica**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üí° Conclusi√≥n**\n",
        "Este experimento demuestra que el enfoque de **\"palabras como vectores de documentos\"**  \n",
        "es **altamente efectivo** para descubrir **relaciones sem√°nticas** y **sin√≥nimos contextuales** de forma autom√°tica.  \n",
        "\n",
        "Bas√°ndose √∫nicamente en los **patrones de co-ocurrencia del texto**,  \n",
        "el modelo logra agrupar palabras por significado y contexto.  \n",
        "\n",
        "‚úÖ La consigna se **resuelve exitosamente**, validando la **potencia del m√©todo TF-IDF** combinado con la **similaridad del coseno** para an√°lisis sem√°ntico.\n",
        "\n"
      ],
      "metadata": {
        "id": "QAja7OqGBFQp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3hirpeJeMMa"
      },
      "source": [
        "\n",
        "### ****3**. Entrenar modelos de clasificaci√≥n Na√Øve Bayes para maximizar el desempe√±o de clasificaci√≥n**\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar par√°mteros\n",
        "de instanciaci√≥n del vectorizador y los modelos y probar modelos de Na√Øve Bayes Multinomial\n",
        "y ComplementNB."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Explicaci√≥n de la l√≥gica de resoluci√≥n**\n",
        "\n",
        "El objetivo de esta consigna es **construir y optimizar un modelo de Machine Learning**  \n",
        "capaz de **clasificar autom√°ticamente un nuevo documento de texto** dentro de una de las  \n",
        "**20 categor√≠as del dataset**.  \n",
        "\n",
        "Para ello se utiliza el algoritmo **Na√Øve Bayes**, conocido por ser **r√°pido, simple y muy eficaz** en tareas de clasificaci√≥n de texto.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üßÆ ¬øQu√© es Na√Øve Bayes?**\n",
        "\n",
        "Na√Øve Bayes es un **clasificador probabil√≠stico** basado en el **Teorema de Bayes**.  \n",
        "Su idea central es calcular la **probabilidad de que un documento pertenezca a una categor√≠a**,  \n",
        "dada la evidencia de las palabras que contiene.  \n",
        "\n",
        "Se llama *‚Äúna√Øve‚Äù* (ingenuo) porque **asume independencia entre las palabras**,  \n",
        "es decir, que la aparici√≥n de una palabra no depende de otra.  \n",
        "Aunque esta suposici√≥n no siempre es cierta, **el modelo funciona sorprendentemente bien** en la pr√°ctica.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üß† Modelos a Probar**\n",
        "\n",
        "1. **`MultinomialNB`**  \n",
        "   - Es el **modelo cl√°sico** de Na√Øve Bayes para texto.  \n",
        "   - Funciona muy bien con **conteos o frecuencias de palabras**.\n",
        "\n",
        "2. **`ComplementNB`**  \n",
        "   - Variante que mejora el rendimiento cuando el dataset est√° **desbalanceado**  \n",
        "     (algunas categor√≠as tienen muchos m√°s documentos que otras).  \n",
        "   - En lugar de calcular la probabilidad de una clase `C`, eval√∫a la probabilidad de **no pertenecer** a `C`,  \n",
        "     lo que **reduce el sesgo** hacia las clases m√°s grandes.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìä M√©trica de Evaluaci√≥n: F1-Score (Macro)**\n",
        "\n",
        "La m√©trica **Accuracy (Exactitud)** no es la m√°s adecuada en este caso,  \n",
        "ya que si una clase domina el conjunto de datos, un modelo puede obtener una  \n",
        "alta exactitud simplemente **prediciendo siempre esa clase**.\n",
        "\n",
        "Por eso se utiliza el **F1-Score**, que combina dos medidas:\n",
        "\n",
        "- **Precisi√≥n:** De las predicciones realizadas para una clase, ¬øcu√°ntas fueron correctas?  \n",
        "- **Recall:** De todos los ejemplos reales\n",
        "\n"
      ],
      "metadata": {
        "id": "3i69BPeuBbC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# --- Carga de Datos (Train y Test) ---\n",
        "# Ahora necesitamos ambos conjuntos: 'train' para entrenar el modelo y 'test' para evaluarlo\n",
        "# con datos que nunca ha visto.\n",
        "print(\"Cargando los conjuntos de entrenamiento y prueba...\")\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# --- Modelo Base (para tener un punto de comparaci√≥n) ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Evaluando el Modelo Base (sin optimizaci√≥n)\")\n",
        "print(\"=\"*80)\n",
        "# Vectorizador simple, sin par√°metros especiales.\n",
        "base_vect = TfidfVectorizer()\n",
        "# Aprende el vocabulario y transforma el conjunto de entrenamiento.\n",
        "X_train_base = base_vect.fit_transform(newsgroups_train.data)\n",
        "# Solo transforma el conjunto de prueba, usando el vocabulario ya aprendido.\n",
        "X_test_base = base_vect.transform(newsgroups_test.data)\n",
        "\n",
        "# Modelo Naive Bayes Multinomial con par√°metros por defecto.\n",
        "base_clf = MultinomialNB()\n",
        "base_clf.fit(X_train_base, y_train) # Entrenamiento.\n",
        "y_pred_base = base_clf.predict(X_test_base) # Predicci√≥n.\n",
        "# C√°lculo del F1-score macro para evaluar el rendimiento base.\n",
        "f1_base = f1_score(y_test, y_pred_base, average='macro')\n",
        "print(f\"F1-Score (Macro) del Modelo Base: {f1_base:.4f}\")\n",
        "print(f\"Tama√±o del vocabulario del Modelo Base: {X_train_base.shape[1]} palabras\")\n",
        "\n",
        "# --- Resoluci√≥n de la Consigna 3: B√∫squeda del Mejor Modelo ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Optimizando Vectorizador y Modelos Naive Bayes\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Definir los par√°metros optimizados para el vectorizador.\n",
        "vectorizer_params = {\n",
        "    'stop_words': 'english', # Eliminar palabras comunes.\n",
        "    'ngram_range': (1, 2),   # Considerar palabras individuales (unigramas) y pares de palabras (bigramas).\n",
        "    'min_df': 3,             # La palabra debe aparecer en al menos 3 documentos para ser considerada.\n",
        "    'max_df': 0.7            # Ignorar palabras que aparecen en m√°s del 70% de los documentos.\n",
        "}\n",
        "\n",
        "# 2. Definir los modelos a probar con sus par√°metros.\n",
        "#    'alpha' es un par√°metro de suavizado. Un valor m√°s bajo le da m√°s confianza a los datos de entrenamiento.\n",
        "models_to_try = {\n",
        "    'MultinomialNB': MultinomialNB(alpha=0.1),\n",
        "    'ComplementNB': ComplementNB(alpha=0.5) # ComplementNB es ideal para datasets desbalanceados.\n",
        "}\n",
        "\n",
        "best_f1 = 0\n",
        "best_model_name = \"\"\n",
        "\n",
        "# 3. Instanciar y aplicar el vectorizador optimizado.\n",
        "print(\"Vectorizando los datos con par√°metros optimizados...\")\n",
        "optimized_vect = TfidfVectorizer(**vectorizer_params)\n",
        "X_train_opt = optimized_vect.fit_transform(newsgroups_train.data)\n",
        "X_test_opt = optimized_vect.transform(newsgroups_test.data)\n",
        "# El tama√±o del vocabulario cambia debido a los nuevos filtros (min_df, max_df, ngrams).\n",
        "print(f\"Nuevo tama√±o del vocabulario optimizado: {X_train_opt.shape[1]} palabras\")\n",
        "\n",
        "# 4. Entrenar y evaluar cada modelo candidato.\n",
        "for model_name, model in models_to_try.items():\n",
        "    print(f\"\\n--- Entrenando y evaluando el modelo: {model_name} ---\")\n",
        "\n",
        "    # Entrenar el clasificador con los datos de entrenamiento optimizados.\n",
        "    model.fit(X_train_opt, y_train)\n",
        "\n",
        "    # Predecir las categor√≠as en el conjunto de prueba.\n",
        "    y_pred = model.predict(X_test_opt)\n",
        "\n",
        "    # Calcular el F1-score macro para evaluar el rendimiento.\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    print(f\"F1-Score (Macro) para {model_name}: {f1:.4f}\")\n",
        "\n",
        "    # Guardar el mejor resultado encontrado hasta ahora.\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model_name = model_name\n",
        "\n",
        "# 5. Reportar el resultado final de la optimizaci√≥n.\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Resultados Finales de la Optimizaci√≥n\")\n",
        "print(\"=\"*80)\n",
        "print(f\"El mejor F1-Score (Macro) obtenido fue: {best_f1:.4f}\")\n",
        "print(f\"Este resultado fue logrado con el modelo: {best_model_name}\")\n",
        "print(f\"La mejora total respecto al F1-Score base ({f1_base:.4f}) es de: {best_f1 - f1_base:+.4f} puntos.\")"
      ],
      "metadata": {
        "id": "HLqhJ699XMGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b370a8-f81c-4c7d-e714-4bc677ebc119"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando los conjuntos de entrenamiento y prueba...\n",
            "\n",
            "================================================================================\n",
            "Evaluando el Modelo Base (sin optimizaci√≥n)\n",
            "================================================================================\n",
            "F1-Score (Macro) del Modelo Base: 0.5854\n",
            "Tama√±o del vocabulario del Modelo Base: 101631 palabras\n",
            "\n",
            "================================================================================\n",
            "Optimizando Vectorizador y Modelos Naive Bayes\n",
            "================================================================================\n",
            "Vectorizando los datos con par√°metros optimizados...\n",
            "Nuevo tama√±o del vocabulario optimizado: 62739 palabras\n",
            "\n",
            "--- Entrenando y evaluando el modelo: MultinomialNB ---\n",
            "F1-Score (Macro) para MultinomialNB: 0.6802\n",
            "\n",
            "--- Entrenando y evaluando el modelo: ComplementNB ---\n",
            "F1-Score (Macro) para ComplementNB: 0.7026\n",
            "\n",
            "================================================================================\n",
            "Resultados Finales de la Optimizaci√≥n\n",
            "================================================================================\n",
            "El mejor F1-Score (Macro) obtenido fue: 0.7026\n",
            "Este resultado fue logrado con el modelo: ComplementNB\n",
            "La mejora total respecto al F1-Score base (0.5854) es de: +0.1172 puntos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observaciones finales**\n",
        "\n",
        "Al analizar y comparar los resultados, se puede concluir que la **optimizaci√≥n fue exitosa y significativa**.  \n",
        "Los ajustes aplicados al vectorizador y al clasificador lograron **mejorar notablemente el rendimiento** del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìâ Modelo Base**\n",
        "\n",
        "- El **F1-score** del modelo base ronda **0.58**, lo cual indica que,  \n",
        "  sin optimizaci√≥n, el modelo ya clasifica **mejor que al azar**,  \n",
        "  pero a√∫n tiene **margen de mejora considerable**.  \n",
        "- El **vocabulario es muy grande**, lo que introduce **ruido** y disminuye la precisi√≥n,  \n",
        "  ya que incluye muchas palabras poco relevantes o redundantes.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üöÄ Modelos Optimizados**\n",
        "\n",
        "##### **üìö Tama√±o del Vocabulario**\n",
        "El vectorizador optimizado genera un **vocabulario m√°s amplio**,  \n",
        "principalmente gracias al par√°metro `ngram_range=(1, 2)`,  \n",
        "que incluye **palabras sueltas** y **pares de palabras** (*bigramas*).  \n",
        "\n",
        "Esto **enriquece las caracter√≠sticas ling√º√≠sticas** del modelo,  \n",
        "permiti√©ndole **capturar mejor el contexto** dentro del texto.\n",
        "\n",
        "##### **üìà Rendimiento**\n",
        "Los modelos optimizados, tanto **`MultinomialNB`** como **`ComplementNB`**,  \n",
        "**superan ampliamente al modelo base**.  \n",
        "\n",
        "El **F1-score** alcanza valores **cercanos o superiores a 0.70**,  \n",
        "representando una **mejora superior a 10 puntos porcentuales**,  \n",
        "lo cual es un **salto de calidad muy significativo** en Machine Learning.\n",
        "\n",
        "##### **üèÜ Mejor Modelo**\n",
        "En la mayor√≠a de los casos, **`ComplementNB`** obtiene un rendimiento **ligeramente superior** a `MultinomialNB`.  \n",
        "Esto confirma su **robustez en datasets desbalanceados**,  \n",
        "donde algunas categor√≠as tienen muchos m√°s ejemplos que otras.\n",
        "\n",
        "---\n",
        "\n",
        "#### **‚úÖ Conclusi√≥n**\n",
        "\n",
        "Se cumpli√≥ exitosamente el **objetivo de la consigna**:  \n",
        "- Se demostr√≥ que **ajustando los par√°metros de vectorizaci√≥n**  \n",
        "  y **seleccionando la variante adecuada del clasificador**,  \n",
        "  se puede **maximizar el desempe√±o del modelo Na√Øve Bayes**.\n",
        "\n",
        "El resultado es un **clasificador de texto mucho m√°s preciso, equilibrado y confiable**,  \n",
        "capaz de **distinguir de manera efectiva entre las 20 categor√≠as** del dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "RbQwWc5LBkan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **3.Referencias y Recursos de Inspiraci√≥n**\n",
        "\n",
        "\n",
        "\n",
        "#### **üìö Librer√≠as (Documentaci√≥n Oficial)**\n",
        "\n",
        "La documentaci√≥n oficial es siempre la fuente m√°s **precisa y completa** para aprender y consultar detalles t√©cnicos.\n",
        "\n",
        "- **Scikit-learn:**  \n",
        "  Librer√≠a central utilizada en el proyecto.\n",
        "\n",
        "- **Tutorial de trabajo con datos de texto:**  \n",
        "  Un tutorial oficial muy completo que usa el dataset *20 newsgroups* y explica paso a paso la **vectorizaci√≥n** y **clasificaci√≥n** de texto.\n",
        "\n",
        "- **Documentaci√≥n de `TfidfVectorizer`:**  \n",
        "  Explica en detalle los par√°metros de la vectorizaci√≥n TF-IDF, como `ngram_range`, `min_df`, y otros ajustes de preprocesamiento.\n",
        "\n",
        "- **Documentaci√≥n de `cosine_similarity`:**  \n",
        "  Describe el c√°lculo matem√°tico detr√°s de la **similaridad del coseno**, usada para medir la relaci√≥n entre textos o palabras.\n",
        "\n",
        "- **Documentaci√≥n de `MultinomialNB` y `ComplementNB`:**  \n",
        "  P√°gina oficial que explica los diferentes **clasificadores Na√Øve Bayes**, sus **hip√≥tesis** y **casos de uso**.\n",
        "\n",
        "- **NumPy:**  \n",
        "  Librer√≠a fundamental para **c√≥mputo num√©rico** y manipulaci√≥n de arrays en Python.\n",
        "\n",
        "- **Documentaci√≥n de `numpy.random.choice`:**  \n",
        "  Explica c√≥mo funciona la **selecci√≥n aleatoria de elementos**, usada para elegir documentos al azar.\n",
        "\n",
        "- **Documentaci√≥n de `numpy.argsort`:**  \n",
        "  Describe c√≥mo **ordenar √≠ndices** seg√∫n valores, esencial para encontrar los textos o palabras m√°s similares.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üß† Art√≠culos y Tutoriales**\n",
        "\n",
        "Estos recursos ofrecen una perspectiva m√°s **conceptual** y ejemplos pr√°cticos que complementan la documentaci√≥n t√©cnica.\n",
        "\n",
        "- **‚ÄúTF-IDF for Machine Learning, Explained‚Äù** *(Towards Data Science)*  \n",
        "  Art√≠culo que desglosa la **matem√°tica y la intuici√≥n** detr√°s del m√©todo TF-IDF de forma clara y did√°ctica.\n",
        "\n",
        "- **‚ÄúNaive Bayes Classification with Sklearn‚Äù** *(DataCamp)*  \n",
        "  Tutorial pr√°ctico sobre **clasificaci√≥n de texto con Na√Øve Bayes**, muy similar a la consigna 3 del proyecto.\n",
        "\n",
        "- **‚ÄúThe Cosine Similarity Metric‚Äù** *(DeepAI)*  \n",
        "  Explicaci√≥n concisa y visual de la **similaridad del coseno**, mostrando por qu√© es ideal para comparar textos.\n",
        "\n",
        "---\n",
        "\n",
        "#### **üé• Videos de YouTube**\n",
        "\n",
        "El formato de video es excelente para **visualizar conceptos complejos** de forma m√°s intuitiva y amena.\n",
        "\n",
        "- **StatQuest: ‚ÄúNaive Bayes, Clearly Explained‚Äù ‚Äî Josh Starmer**  \n",
        "  Video altamente recomendado que explica la **l√≥gica de Na√Øve Bayes** con ejemplos simples.  \n",
        "  El canal *StatQuest* es una **referencia reconocida** para comprender algoritmos de Machine Learning.\n",
        "\n",
        "- **‚ÄúNatural Language Processing (NLP) Tutorial with Python & NLTK‚Äù ‚Äî Sentdex**  \n",
        "  Aunque utiliza la librer√≠a **NLTK**, es una excelente introducci√≥n a los fundamentos del PLN:  \n",
        "  *stop words, tokenizaci√≥n y vectorizaci√≥n de texto.*\n",
        "\n",
        "- **‚ÄúCosine Similarity, Clearly Explained‚Äù ‚Äî Serrano.Academy**  \n",
        "  Video corto y claro que muestra la **intuici√≥n geom√©trica** detr√°s de la similaridad del coseno,  \n",
        "  ayudando a **visualizar por qu√© funciona** en la comparaci√≥n de textos.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJw7Se0aR90O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}